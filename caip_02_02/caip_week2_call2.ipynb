{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763c0e9a-2387-493f-94bc-96ffecd4a9c8",
   "metadata": {},
   "source": [
    "## Certified AI Practitioner Week 02 Call 02 - Deploy and Test Your First ML Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84b897-7cba-4d9e-920f-ca6193b7e4ad",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880bca35-6340-4db1-9b2b-760da3539d1f",
   "metadata": {},
   "source": [
    "- Explain the difference between real-time, batch, and async inference in SageMaker  \n",
    "- Locate and load a trained model artifact from S3  \n",
    "- Deploy a scikit-learn model to a real-time SageMaker endpoint  \n",
    "- Send a test payload using `boto3` and interpret the prediction result  \n",
    "- Access and review endpoint logs using CloudWatch  \n",
    "- Identify common errors and performance metrics in deployed inference  \n",
    "- Delete SageMaker endpoints to avoid unnecessary cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6b15f",
   "metadata": {},
   "source": [
    "## Infrastructure Setup with CloudFormation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1a9b6",
   "metadata": {},
   "source": [
    "To train models in SageMaker Studio, we first need to provision the necessary infrastructure.\n",
    "\n",
    "In this step, we‚Äôll use an automated **CloudFormation template** to create:\n",
    "\n",
    "- A **SageMaker Studio domain** for running cloud-based notebooks  \n",
    "- An **IAM execution role** with S3 and SageMaker permissions  \n",
    "- A dedicated **S3 bucket** to store training data and model artifacts  \n",
    "\n",
    "Instead of clicking through the AWS Console, we‚Äôll deploy this setup programmatically using `boto3`. The stack will output everything you need - including the bucket name and role ARN - to use in the next steps of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f658ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating stack: caip02-cloud-ml-stack\n",
      "‚è≥ Waiting for caip02-cloud-ml-stack to complete...\n",
      "‚úÖ Stack operation completed.\n",
      "üîß Stack Outputs:\n",
      "{\n",
      "  \"StudioUserName\": \"caip02-user\",\n",
      "  \"BucketName\": \"caip02-ml-bucket-zali\",\n",
      "  \"DomainId\": \"d-dub50qaijyte\",\n",
      "  \"RoleArn\": \"arn:aws:iam::458806987020:role/caip02-execution-role-zali\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "def stack_exists(name):\n",
    "    try:\n",
    "        cf.describe_stacks(StackName=name)\n",
    "        return True\n",
    "    except cf.exceptions.ClientError as e:\n",
    "        if \"does not exist\" in str(e):\n",
    "            return False\n",
    "        raise  # re-raise any unexpected error\n",
    "\n",
    "def deploy_stack(stack_name, template_body, parameters):\n",
    "    if stack_exists(stack_name):\n",
    "        print(f\"üîÑ Updating stack: {stack_name}\")\n",
    "        try:\n",
    "            response = cf.update_stack(\n",
    "                StackName=stack_name,\n",
    "                TemplateBody=template_body,\n",
    "                Parameters=parameters,\n",
    "                Capabilities=[\"CAPABILITY_NAMED_IAM\"]\n",
    "            )\n",
    "            waiter = cf.get_waiter(\"stack_update_complete\")\n",
    "        except cf.exceptions.ClientError as e:\n",
    "            if \"No updates are to be performed\" in str(e):\n",
    "                print(\"‚úÖ No changes detected.\")\n",
    "                # Print outputs\n",
    "                outputs = cf.describe_stacks(StackName=stack_name)[\"Stacks\"][0][\"Outputs\"]\n",
    "                print(\"üîß Stack Outputs:\")\n",
    "                print(json.dumps({o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs}, indent=2))\n",
    "                return outputs\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        print(f\"üöÄ Creating stack: {stack_name}\")\n",
    "        response = cf.create_stack(\n",
    "            StackName=stack_name,\n",
    "            TemplateBody=template_body,\n",
    "            Parameters=parameters,\n",
    "            Capabilities=[\"CAPABILITY_NAMED_IAM\"]\n",
    "        )\n",
    "        waiter = cf.get_waiter(\"stack_create_complete\")\n",
    "\n",
    "    print(f\"‚è≥ Waiting for {stack_name} to complete...\")\n",
    "    waiter.wait(StackName=stack_name)\n",
    "    print(\"‚úÖ Stack operation completed.\")\n",
    "\n",
    "    # Print outputs\n",
    "    outputs = cf.describe_stacks(StackName=stack_name)[\"Stacks\"][0][\"Outputs\"]\n",
    "    print(\"üîß Stack Outputs:\")\n",
    "    print(json.dumps({o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs}, indent=2))\n",
    "    return outputs\n",
    "\n",
    "ec2 = boto3.client(\"ec2\")\n",
    "cf = boto3.client(\"cloudformation\")\n",
    "\n",
    "# Get the default VPC ID\n",
    "vpc_id = ec2.describe_vpcs(Filters=[{\"Name\": \"isDefault\", \"Values\": [\"true\"]}])[\"Vpcs\"][0][\"VpcId\"]\n",
    "\n",
    "# Get a public subnet ID in that VPC\n",
    "subnets = ec2.describe_subnets(Filters=[{\"Name\": \"vpc-id\", \"Values\": [vpc_id]}])\n",
    "subnet_id = subnets[\"Subnets\"][0][\"SubnetId\"]\n",
    "\n",
    "# Load your template\n",
    "with open(\"cf_templates/sagemaker_infra.yaml\") as f:\n",
    "    template_body = f.read()\n",
    "\n",
    "bucketNameSuffix = \"zali\"\n",
    "stack_name = \"caip02-cloud-ml-stack\"\n",
    "\n",
    "parameters = [\n",
    "    {\"ParameterKey\": \"BucketNameSuffix\", \"ParameterValue\": bucketNameSuffix},\n",
    "    {\"ParameterKey\": \"VpcId\", \"ParameterValue\": vpc_id},\n",
    "    {\"ParameterKey\": \"SubnetId\", \"ParameterValue\": subnet_id}\n",
    "]\n",
    "\n",
    "outputs = deploy_stack(stack_name, template_body, parameters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb83a73",
   "metadata": {},
   "source": [
    "## Upload Data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df739d9a",
   "metadata": {},
   "source": [
    "We‚Äôll upload the previously prepared Titanic dataset to our dedicated S3 bucket so it can be used by SageMaker for training.\n",
    "\n",
    "Make sure you‚Äôre using the bucket created by your CloudFormation stack. You can retrieve it from the stack outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3881277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded cleaned_titanic.csv to s3://caip02-ml-bucket-zali/inputs/cleaned_titanic.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "bucket_name = {o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs if o[\"OutputKey\"] == \"BucketName\"}[\"BucketName\"]\n",
    "\n",
    "key = \"inputs/cleaned_titanic.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(\"cleaned_titanic.csv\", bucket_name, key)\n",
    "\n",
    "print(f\"Uploaded cleaned_titanic.csv to s3://{bucket_name}/{key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf3d80",
   "metadata": {},
   "source": [
    "## Write the Training Script (train_model.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51353d",
   "metadata": {},
   "source": [
    "SageMaker training jobs run inside isolated containers. Instead of writing code cell by cell, we‚Äôll create a standalone script that SageMaker will execute in the cloud.\n",
    "\n",
    "This script will:\n",
    "\n",
    "- Load the Titanic dataset from the input channel\n",
    "- Train a Decision Tree model\n",
    "- Evaluate performance using a confusion matrix and classification report\n",
    "- Print feature importances\n",
    "- Save the trained model to the `/opt/ml/model/` directory for SageMaker to capture\n",
    "\n",
    "We‚Äôll save this script locally so we can pass it into a SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83c8d1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import pandas as pd\n",
       "import joblib\n",
       "import os\n",
       "\n",
       "from sklearn.tree import DecisionTreeClassifier\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.preprocessing import StandardScaler\n",
       "from sklearn.metrics import classification_report, confusion_matrix\n",
       "\n",
       "def main():\n",
       "    # SageMaker passes input as /opt/ml/input/data/train/\n",
       "    input_path = \"/opt/ml/input/data/train/cleaned_titanic.csv\"\n",
       "    df = pd.read_csv(input_path)\n",
       "\n",
       "    # Split features and target\n",
       "    X = df.drop(\"Survived\", axis=1)\n",
       "    y = df[\"Survived\"]\n",
       "\n",
       "    # Train/test split\n",
       "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
       "\n",
       "    # Scale features\n",
       "    scaler = StandardScaler()\n",
       "    X_train_scaled = scaler.fit_transform(X_train)\n",
       "    X_test_scaled = scaler.transform(X_test)\n",
       "\n",
       "    # Train model\n",
       "    clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
       "    clf.fit(X_train_scaled, y_train)\n",
       "\n",
       "    # Evaluate\n",
       "    y_pred = clf.predict(X_test_scaled)\n",
       "\n",
       "    print(\"\\n=== Classification Report ===\\n\")\n",
       "    print(classification_report(y_test, y_pred))\n",
       "\n",
       "    print(\"\\n=== Confusion Matrix ===\\n\")\n",
       "    print(confusion_matrix(y_test, y_pred))\n",
       "\n",
       "    print(\"\\n=== Feature Importances ===\\n\")\n",
       "    importances = pd.Series(clf.feature_importances_, index=X.columns)\n",
       "    print(importances.sort_values(ascending=False))\n",
       "\n",
       "    # Save model\n",
       "    os.makedirs(\"/opt/ml/model\", exist_ok=True)\n",
       "    joblib.dump(clf, \"/opt/ml/model/model.joblib\")\n",
       "    joblib.dump(scaler, \"/opt/ml/model/scaler.joblib\")\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def show_code(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        code = f.read()\n",
    "    display(Markdown(f\"```python\\n{code}\\n```\"))\n",
    "\n",
    "# Call this after writing the file\n",
    "show_code(\"train_model.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f755f",
   "metadata": {},
   "source": [
    "## Launch a SageMaker Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac323e8e",
   "metadata": {},
   "source": [
    "We‚Äôll use the **SKLearn Estimator**, which runs our `train_model.py` script that we previously developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce844f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Ziggy\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: titanic-decision-tree-2025-09-11-22-47-57-071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-11 22:47:59 Starting - Starting the training job...\n",
      "2025-09-11 22:48:37 Downloading - Downloading input data...\n",
      "2025-09-11 22:49:02 Downloading - Downloading the training image.....2025-09-11 22:50:08,167 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\n",
      "2025-09-11 22:50:08,171 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2025-09-11 22:50:08,174 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-09-11 22:50:08,191 sagemaker_sklearn_container.training INFO     Invoking user training script.\n",
      "2025-09-11 22:50:08,485 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2025-09-11 22:50:08,489 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-09-11 22:50:08,509 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2025-09-11 22:50:08,512 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-09-11 22:50:08,533 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2025-09-11 22:50:08,536 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-09-11 22:50:08,553 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.large\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.large\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"titanic-decision-tree-2025-09-11-22-47-57-071\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://caip02-ml-bucket-zali/titanic-decision-tree-2025-09-11-22-47-57-071/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_model\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"train_model.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={}\n",
      "SM_USER_ENTRY_POINT=train_model.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\n",
      "SM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.m5.large\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=train_model\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=2\n",
      "SM_NUM_GPUS=0\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://caip02-ml-bucket-zali/titanic-decision-tree-2025-09-11-22-47-57-071/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.large\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"titanic-decision-tree-2025-09-11-22-47-57-071\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://caip02-ml-bucket-zali/titanic-decision-tree-2025-09-11-22-47-57-071/source/sourcedir.tar.gz\",\"module_name\":\"train_model\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"train_model.py\"}\n",
      "SM_USER_ARGS=[]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "PYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\n",
      "Invoking script with the following command:\n",
      "/miniconda3/bin/python train_model.py\n",
      "2025-09-11 22:50:08,554 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\n",
      "2025-09-11 22:50:08,555 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "         0.0       0.80      0.90      0.84       106\n",
      "         1.0       0.81      0.67      0.73        72\n",
      "    accuracy                           0.80       178\n",
      "   macro avg       0.81      0.78      0.79       178\n",
      "weighted avg       0.80      0.80      0.80       178\n",
      "=== Confusion Matrix ===\n",
      "[[95 11]\n",
      " [24 48]]\n",
      "=== Feature Importances ===\n",
      "Sex         0.592333\n",
      "Pclass      0.266138\n",
      "Age         0.074534\n",
      "Fare        0.042353\n",
      "SibSp       0.024641\n",
      "Embarked    0.000000\n",
      "Parch       0.000000\n",
      "dtype: float64\n",
      "2025-09-11 22:50:09,615 sagemaker-containers INFO     Reporting training SUCCESS\n",
      "\n",
      "2025-09-11 22:50:31 Training - Training image download completed. Training in progress.\n",
      "2025-09-11 22:50:31 Uploading - Uploading generated training model\n",
      "2025-09-11 22:50:31 Completed - Training job completed\n",
      "Training seconds: 114\n",
      "Billable seconds: 114\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# Values from your CloudFormation stack\n",
    "bucket = {o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs if o[\"OutputKey\"] == \"BucketName\"}[\"BucketName\"]\n",
    "role = {o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs if o[\"OutputKey\"] == \"RoleArn\"}[\"RoleArn\"]\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Input and output locations in S3\n",
    "input_path = f\"s3://{bucket}/inputs/\"\n",
    "output_path = f\"s3://{bucket}/models/\"\n",
    "\n",
    "# Set up the SageMaker session\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Create SKLearn estimator\n",
    "estimator = SKLearn(\n",
    "    entry_point=\"train_model.py\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    framework_version=\"1.0-1\",\n",
    "    sagemaker_session=session,\n",
    "    output_path=output_path,\n",
    "    base_job_name=\"titanic-decision-tree\"\n",
    ")\n",
    "\n",
    "# Launch the training job\n",
    "estimator.fit({\"train\": input_path})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674a4d57",
   "metadata": {},
   "source": [
    "## Intro to Inference in SageMaker\n",
    "\n",
    "After training a model, the next step is **inference** ‚Äî making predictions from new, unseen data. SageMaker provides multiple deployment options, each suited for different use cases:\n",
    "\n",
    "---\n",
    "\n",
    "### Inference Options\n",
    "\n",
    "| Option       | Use Case                                      |\n",
    "|--------------|-----------------------------------------------|\n",
    "| **Real-Time Endpoint** | Instant predictions via API calls (e.g., fraud detection, recommendation systems) |\n",
    "| **Batch Transform**    | Run predictions on large datasets stored in S3 (e.g., nightly scoring jobs) |\n",
    "| **Async Inference**    | Queue-based, low-latency jobs for large or complex inputs (e.g., documents, images) |\n",
    "\n",
    "---\n",
    "\n",
    "In this call, we'll focus on **real-time inference** using a SageMaker-hosted endpoint. This will:\n",
    "\n",
    "- Load your trained model into memory\n",
    "- Expose it as a live HTTPS endpoint\n",
    "- Accept and respond to `InvokeEndpoint` requests with predictions\n",
    "\n",
    "You'll also learn how to monitor inference activity in **CloudWatch Logs** and how to delete the endpoint to avoid unnecessary charges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4053e",
   "metadata": {},
   "source": [
    "### Real-Time Inference Architecture\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"inference_pipeline.png\" alt=\"SageMaker Real-Time Inference Flow\" style=\"width:70%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe42a16",
   "metadata": {},
   "source": [
    "## Load the Trained Model Artifact from S3\n",
    "\n",
    "Before we can deploy a model to a SageMaker endpoint, we need to locate the **trained model artifact** that was saved during our previous training job.\n",
    "\n",
    "When you ran the `SKLearn` Estimator in Call 1, SageMaker saved your model (as a `.tar.gz` archive) to the **S3 output path** you specified, typically something like:\n",
    "\n",
    "```yaml\n",
    "s3://your-bucket-name/models/<training-job-name>/output/model.tar.gz\n",
    "```\n",
    "\n",
    "\n",
    "This file contains the serialized model object (e.g., a `joblib` or `pickle` dump), and SageMaker uses it to load the model into the deployed container.\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we‚Äôll:\n",
    "\n",
    "- Locate your model artifact in S3  \n",
    "- Confirm that it exists and is ready to deploy  \n",
    "- (Optionally) download and inspect it locally to verify the contents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff233154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found latest model artifact: s3://caip02-ml-bucket-zali/models/titanic-decision-tree-2025-09-11-22-47-57-071/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# If using CloudFormation outputs\n",
    "bucket_name = {o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs if o[\"OutputKey\"] == \"BucketName\"}[\"BucketName\"]\n",
    "model_prefix = \"models/\"  # Adjust if needed\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# List all model artifacts under the prefix\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=model_prefix)\n",
    "\n",
    "# Filter to only model.tar.gz files\n",
    "model_files = [\n",
    "    obj for obj in response.get(\"Contents\", [])\n",
    "    if obj[\"Key\"].endswith(\"model.tar.gz\")\n",
    "]\n",
    "\n",
    "# Sort by last modified date (latest first)\n",
    "model_files_sorted = sorted(model_files, key=lambda x: x[\"LastModified\"], reverse=True)\n",
    "\n",
    "# Get the most recent one\n",
    "if model_files_sorted:\n",
    "    model_artifact = model_files_sorted[0][\"Key\"]\n",
    "    print(f\"Found latest model artifact: s3://{bucket_name}/{model_artifact}\")\n",
    "else:\n",
    "    print(\"No model.tar.gz files found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89518350",
   "metadata": {},
   "source": [
    "## Write the Inference Script (`inference.py`)\n",
    "\n",
    "By default, SageMaker expects your deployed model to accept preprocessed input. But in our case, the model was trained on **scaled data**, and the raw input we want to send (e.g., `[Pclass, Sex, Age, ...]`) hasn't been scaled yet.\n",
    "\n",
    "To solve this, we‚Äôll bundle a custom `inference.py` script with our model that:\n",
    "\n",
    "- Loads both the trained model and the fitted `StandardScaler`  \n",
    "- Scales the incoming input using the same logic as training  \n",
    "- Makes and returns predictions  \n",
    "\n",
    "This script runs **inside the container** at inference time and ensures your endpoint can accept **raw, unscaled data** directly.\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Need\n",
    "\n",
    "1. A copy of the fitted `StandardScaler` saved as `scaler.joblib`  \n",
    "2. A trained model saved as `model.joblib`  \n",
    "3. A custom script named `inference.py` that defines:\n",
    "   - `model_fn(model_dir)` ‚Äî loads model and scaler  \n",
    "   - `predict_fn(input_data, model_and_scaler)` ‚Äî preprocesses and returns prediction\n",
    "\n",
    "We'll deploy this using the `entry_point` + `source_dir` parameters in the `SKLearnModel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "027c51a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import joblib\n",
       "import numpy as np\n",
       "import os\n",
       "import json\n",
       "\n",
       "# Called when the model is loaded\n",
       "def model_fn(model_dir):\n",
       "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
       "    scaler = joblib.load(os.path.join(model_dir, \"scaler.joblib\"))\n",
       "    return (model, scaler)\n",
       "\n",
       "# Called when a prediction is made\n",
       "def predict_fn(input_data, model_and_scaler):\n",
       "    model, scaler = model_and_scaler\n",
       "\n",
       "    # Ensure input is a NumPy array\n",
       "    if isinstance(input_data, list):\n",
       "        input_data = np.array(input_data)\n",
       "\n",
       "    # Apply scaling\n",
       "    scaled = scaler.transform(input_data)\n",
       "\n",
       "    # Make prediction\n",
       "    prediction = model.predict(scaled)\n",
       "\n",
       "    # Log everything to CloudWatch\n",
       "    print(\"Input received:\", json.dumps(input_data.tolist()))\n",
       "    print(\"Scaled input:\", json.dumps(scaled.tolist()))\n",
       "    print(\"Prediction result:\", json.dumps(prediction.tolist()))\n",
       "\n",
       "    return prediction.tolist()\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def show_code(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        code = f.read()\n",
    "    display(Markdown(f\"```python\\n{code}\\n```\"))\n",
    "\n",
    "# Call this after writing the file\n",
    "show_code(\"inference.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0feb1ef",
   "metadata": {},
   "source": [
    "## Deploy a Real-Time Endpoint using `SKLearnModel.deploy()`\n",
    "\n",
    "Once we have the trained model artifact in S3, we can deploy it to a real-time endpoint using SageMaker‚Äôs `SKLearnModel` class.\n",
    "\n",
    "This allows us to:\n",
    "\n",
    "- Load the `.tar.gz` model artifact into a container\n",
    "- Launch a managed HTTPS endpoint hosted by SageMaker\n",
    "- Invoke the endpoint with real-time prediction requests\n",
    "\n",
    "---\n",
    "\n",
    "### What Happens Under the Hood\n",
    "\n",
    "1. SageMaker creates an **endpoint configuration** that links the model, container image, and instance type  \n",
    "2. It launches an **endpoint** that pulls the container + model into memory  \n",
    "3. The endpoint becomes a live API you can call with `invoke_endpoint()` from `boto3`\n",
    "\n",
    "---\n",
    "\n",
    "In this step, we‚Äôll:\n",
    "\n",
    "- Define a `SKLearnModel` object using the path to your model artifact in S3  \n",
    "- Deploy it to an endpoint (e.g., `titanic-endpoint`)  \n",
    "- Print the endpoint name so we can use it in the next step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35cab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-scikit-learn-2025-09-11-22-52-04-217\n",
      "INFO:sagemaker:Creating endpoint-config with name titanic-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name titanic-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!Deployed endpoint: titanic-endpoint\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "# Set required parameters\n",
    "bucket_name = {o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs if o[\"OutputKey\"] == \"BucketName\"}[\"BucketName\"]\n",
    "role = {o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs if o[\"OutputKey\"] == \"RoleArn\"}[\"RoleArn\"]\n",
    "region = boto3.Session().region_name\n",
    "endpoint_name = \"titanic-endpoint\"\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "try:\n",
    "    sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "    sm.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    print(f\"Deleted existing endpoint and config: {endpoint_name}\")\n",
    "except sm.exceptions.ClientError as e:\n",
    "    if \"Could not find\" not in str(e):\n",
    "        raise\n",
    "\n",
    "# Define model object\n",
    "model = SKLearnModel(\n",
    "    model_data=f\"s3://{bucket}/{model_artifact}\",\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.0-1\",\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")\n",
    "\n",
    "\n",
    "# Deploy the model\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=\"titanic-endpoint\"\n",
    ")\n",
    "\n",
    "print(f\"Deployed endpoint: {endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0040ca",
   "metadata": {},
   "source": [
    "## Call the Endpoint\n",
    "\n",
    "Once your model is deployed as a real-time endpoint, you can make predictions by calling it through SageMaker‚Äôs `invoke_endpoint` API.\n",
    "\n",
    "In this step, we‚Äôll:\n",
    "\n",
    "- Prepare a sample payload (one row of Titanic-style input features)\n",
    "- Call the endpoint using `boto3`\n",
    "- Parse the prediction result\n",
    "- (Optional) Display the predicted class and confidence score\n",
    "\n",
    "This simulates what a client app or API gateway would do to request live predictions from your deployed model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256fb1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passenger Info:\n",
      "  Pclass: 3 -> 3rd Class\n",
      "  Sex: 0 -> Male\n",
      "  Age: 22.0 -> 22.0\n",
      "  SibSp: 1 -> 1\n",
      "  Parch: 0 -> 0\n",
      "  Fare: 7.25 -> 7.25\n",
      "  Embarked: 0 -> Southampton (S)\n",
      "\n",
      "Prediction: 0.0000 ‚Üí Class: 0 (Did Not Survive)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def predict_survival(payload, endpoint_name=\"titanic-endpoint\"):\n",
    "    \"\"\"\n",
    "    Sends a payload to the specified SageMaker endpoint and returns the prediction.\n",
    "\n",
    "    Parameters:\n",
    "        payload (list of list): A 2D list representing one or more passengers.\n",
    "            Each row should follow the format:\n",
    "            [Pclass, Sex, Age, SibSp, Parch, Fare, Embarked]\n",
    "        endpoint_name (str): The name of the deployed SageMaker endpoint.\n",
    "\n",
    "    Returns:\n",
    "        float or int: The first predicted value from the endpoint response.\n",
    "        Typically 0.0 or 1.0, or a probability depending on model output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert payload to JSON and call endpoint\n",
    "    runtime = boto3.client(\"sagemaker-runtime\")\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "\n",
    "    # Read and decode response\n",
    "    result = response[\"Body\"].read().decode(\"utf-8\")\n",
    "    prediction = json.loads(result)\n",
    "    return prediction[0]\n",
    "\n",
    "\n",
    "def print_prediction_summary(data_rows, prediction):\n",
    "    \"\"\"\n",
    "    Pretty-prints the passenger data and model prediction.\n",
    "    \n",
    "    Parameters:\n",
    "        data_rows (list): A list of input rows. Each row is a list of:\n",
    "            [Pclass, Sex, Age, SibSp, Parch, Fare, Embarked]\n",
    "        prediction (float): The raw prediction score (e.g., probability of survival).\n",
    "    \"\"\"\n",
    "    columns = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "    # Mappings for readability\n",
    "    pclass_map = {1: \"1st Class\", 2: \"2nd Class\", 3: \"3rd Class\"}\n",
    "    sex_map = {0: \"Male\", 1: \"Female\"}\n",
    "    embarked_map = {0: \"Southampton (S)\", 1: \"Cherbourg (C)\", 2: \"Queenstown (Q)\"}\n",
    "\n",
    "    predicted_class = int(np.round(prediction))\n",
    "\n",
    "    for row in data_rows:\n",
    "        if len(row) != len(columns):\n",
    "            print(\"Invalid data row: must contain 7 values.\")\n",
    "            continue\n",
    "\n",
    "        print(\"Passenger Info:\")\n",
    "        for col, val in zip(columns, row):\n",
    "            readable = val\n",
    "            if col == \"Pclass\":\n",
    "                readable = pclass_map.get(val, f\"Unknown ({val})\")\n",
    "            elif col == \"Sex\":\n",
    "                readable = sex_map.get(val, f\"Unknown ({val})\")\n",
    "            elif col == \"Embarked\":\n",
    "                readable = embarked_map.get(val, f\"Unknown ({val})\")\n",
    "            print(f\"  {col}: {val} -> {readable}\")\n",
    "\n",
    "        outcome = \"Survived\" if predicted_class == 1 else \"Did Not Survive\"\n",
    "        print(f\"\\nPrediction: {prediction:.4f} ‚Üí Class: {predicted_class} ({outcome})\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# Sample input: [Pclass, Sex, Age, SibSp, Parch, Fare, Embarked]\n",
    "# This passenger is a 22-year-old male in 3rd class, traveling alone with a low fare.\n",
    "# Historically, this profile had a lower survival rate on the Titanic.\n",
    "test_passenger = [[3, 0, 22.0, 1, 0, 7.25, 0]]\n",
    "prediction = predict_survival(test_passenger)\n",
    "print_prediction_summary(test_passenger, prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a153fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passenger Info:\n",
      "  Pclass: 1 -> 1st Class\n",
      "  Sex: 0 -> Male\n",
      "  Age: 65.0 -> 65.0\n",
      "  SibSp: 0 -> 0\n",
      "  Parch: 0 -> 0\n",
      "  Fare: 82.0 -> 82.0\n",
      "  Embarked: 1 -> Cherbourg (C)\n",
      "\n",
      "Prediction: 0.0000 ‚Üí Class: 0 (Did Not Survive)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_passenger = [[1, 0, 65.0, 0, 0, 82.0, 1]]  # 1st class, male, 65 y/o, traveling alone\n",
    "prediction = predict_survival(test_passenger)\n",
    "print_prediction_summary(test_passenger, prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "623b4a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passenger Info:\n",
      "  Pclass: 3 -> 3rd Class\n",
      "  Sex: 1 -> Female\n",
      "  Age: 19.0 -> 19.0\n",
      "  SibSp: 0 -> 0\n",
      "  Parch: 0 -> 0\n",
      "  Fare: 7.75 -> 7.75\n",
      "  Embarked: 2 -> Queenstown (Q)\n",
      "\n",
      "Prediction: 1.0000 ‚Üí Class: 1 (Survived)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_passenger = [[3, 1, 19.0, 0, 0, 7.75, 2]]  # 3rd class, female, 19 y/o, low fare\n",
    "prediction = predict_survival(test_passenger)\n",
    "print_prediction_summary(test_passenger, prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb999709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passenger Info:\n",
      "  Pclass: 1 -> 1st Class\n",
      "  Sex: 1 -> Female\n",
      "  Age: 38.0 -> 38.0\n",
      "  SibSp: 1 -> 1\n",
      "  Parch: 1 -> 1\n",
      "  Fare: 153.46 -> 153.46\n",
      "  Embarked: 1 -> Cherbourg (C)\n",
      "\n",
      "Prediction: 1.0000 ‚Üí Class: 1 (Survived)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_passenger = [[1, 1, 38.0, 1, 1, 153.46, 1]]  # 1st class, female, adult with child, high fare\n",
    "prediction = predict_survival(test_passenger)\n",
    "print_prediction_summary(test_passenger, prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9374e7",
   "metadata": {},
   "source": [
    "## Review Logs in CloudWatch\n",
    "\n",
    "Just like with training jobs, SageMaker captures logs from deployed inference endpoints and sends them to **Amazon CloudWatch**. These logs include:\n",
    "\n",
    "- Container startup events  \n",
    "- Any `print()` statements inside your inference script (if using a custom container)  \n",
    "- Internal SageMaker logging (including input/output size, latency, errors)\n",
    "\n",
    "---\n",
    "\n",
    "### How to Access Endpoint Logs\n",
    "\n",
    "1. Go to the [SageMaker Console](https://console.aws.amazon.com/sagemaker/home)\n",
    "2. Click **Endpoints** in the left sidebar\n",
    "3. Select your endpoint name (e.g., `titanic-endpoint`)\n",
    "4. Scroll down to **Monitor**\n",
    "5. Click the **View logs** link next to the production variant\n",
    "6. You'll be redirected to **CloudWatch Logs** where you can inspect activity\n",
    "\n",
    "---\n",
    "\n",
    "### Inference Log Screenshot (3rd class, female, 19 y/o, low fare)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"sagemaker_endpoint_logs.png\" alt=\"SageMaker Endpoint Logs\" style=\"width:80%;\">\n",
    "</div>\n",
    "\n",
    "CloudWatch logs are especially helpful for catching input shape mismatches, serialization issues, or performance bottlenecks.\n",
    "\n",
    "> Pro tip: Set up **CloudWatch Alarms** to alert if an endpoint returns too many errors or takes too long to respond.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b44f1",
   "metadata": {},
   "source": [
    "## Common Errors and Metrics in Endpoint Logs\n",
    "\n",
    "When calling a SageMaker endpoint, your logs in CloudWatch can reveal a lot about what‚Äôs working ‚Äî and what‚Äôs broken.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Inference Errors\n",
    "\n",
    "| Error Type            | Cause / Fix |\n",
    "|-----------------------|-------------|\n",
    "| **ModelError**        | Your script raised an unhandled exception (e.g., input shape mismatch, bad `joblib` file) |\n",
    "| **ValidationError**   | You sent invalid JSON, missing required fields, or wrong `ContentType` |\n",
    "| **InternalFailure**   | A container crash ‚Äî often due to missing files, dependencies, or bad serialization |\n",
    "| **ThrottlingException** | You're sending too many requests for your instance size (scale up or batch inputs) |\n",
    "| **EndpointConnectionError** | Your endpoint was deleted or not yet ready (check deployment status) |\n",
    "\n",
    "---\n",
    "\n",
    "### Metrics to Monitor\n",
    "\n",
    "| Metric                     | What It Tells You                          |\n",
    "|----------------------------|--------------------------------------------|\n",
    "| `ModelLatency`             | Time spent inside the model container (ms) |\n",
    "| `OverheadLatency`          | Time spent outside the container (network, etc.) |\n",
    "| `Invocation4XXErrors`      | Client-side errors (bad input, wrong format) |\n",
    "| `Invocation5XXErrors`      | Server-side errors (model crashed, etc.)   |\n",
    "| `Invocations`              | Number of successful requests              |\n",
    "| `InvocationThrottled`      | You‚Äôre hitting your instance‚Äôs request limit |\n",
    "| `MemoryUtilization`        | If your container is running out of RAM    |\n",
    "\n",
    "You can view these metrics under **SageMaker ‚Üí Endpoints ‚Üí Monitoring** or directly in **CloudWatch Metrics**.\n",
    "\n",
    "---\n",
    "\n",
    "> In production, it‚Äôs best practice to build CloudWatch **Alarms** or **Dashboards** around these metrics ‚Äî especially `Invocation5XXErrors` and `ModelLatency`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26065abb",
   "metadata": {},
   "source": [
    "## Review Metrics in CloudWatch\n",
    "\n",
    "In addition to logs, SageMaker automatically sends a variety of **real-time performance and operational metrics** to **Amazon CloudWatch**.\n",
    "\n",
    "These metrics help you monitor how your endpoint is behaving ‚Äî whether it's receiving traffic, how fast it‚Äôs responding, and whether it's throwing errors.\n",
    "\n",
    "---\n",
    "\n",
    "### How to Access Endpoint Metrics\n",
    "\n",
    "1. Go to the [CloudWatch Console ‚Üí Metrics](https://console.aws.amazon.com/cloudwatch/home#metrics:)\n",
    "2. Choose **SageMaker**\n",
    "3. Select **Endpoints ‚Üí EndpointName, VariantName**\n",
    "4. Choose your endpoint (e.g., `titanic-endpoint`)\n",
    "5. You can now graph metrics and set alarms\n",
    "\n",
    "---\n",
    "\n",
    "### Common Metrics to Monitor\n",
    "\n",
    "| Metric Name              | Description                                       |\n",
    "|--------------------------|---------------------------------------------------|\n",
    "| `Invocations`            | Number of inference requests received             |\n",
    "| `ModelLatency`           | Time (ms) spent running the model in the container|\n",
    "| `OverheadLatency`        | Time spent outside the model (e.g., networking)   |\n",
    "| `Invocation4XXErrors`    | Client errors (bad input, invalid format, etc.)   |\n",
    "| `Invocation5XXErrors`    | Server errors (container crashes, exceptions)     |\n",
    "| `MemoryUtilization`*     | Memory usage of the container (if enabled)        |\n",
    "\n",
    "> *`MemoryUtilization` only shows up if your container is configured to emit resource metrics.\n",
    "\n",
    "---\n",
    "\n",
    "These metrics help you:\n",
    "- Detect performance issues (e.g., slow inference or memory spikes)\n",
    "- Identify input or model errors\n",
    "- Avoid idle costs or unexpected spikes in usage\n",
    "\n",
    "You can also build dashboards or alarms directly from these metrics.\n",
    "\n",
    "### Cloudwatch Metrics Screenshot (ModelLatency)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"sagemaker_metrics.png\" alt=\"SageMaker Endpoint Logs\" style=\"width:80%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4582cb0c",
   "metadata": {},
   "source": [
    "## Delete the Endpoint Using `.delete_endpoint()` or `boto3`\n",
    "\n",
    "SageMaker real-time endpoints are **always-on** and billed by the hour, even if they‚Äôre idle. To avoid unnecessary charges, it‚Äôs important to delete the endpoint when you're done testing.\n",
    "\n",
    "You can delete the endpoint in two ways:\n",
    "\n",
    "- Use the `delete_endpoint()` method from your deployed `predictor` object  \n",
    "- Use the `boto3` SDK to delete it by name\n",
    "\n",
    "We‚Äôll show both methods below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ea1b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing endpoint and config: titanic-endpoint\n",
      "Endpoint 'titanic-endpoint' deleted using boto3.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "try:\n",
    "    sm.delete_endpoint(EndpointName=endpoint_name)\n",
    "    sm.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    print(f\"Deleted existing endpoint and config: {endpoint_name}\")\n",
    "except sm.exceptions.ClientError as e:\n",
    "    if \"Could not find\" not in str(e):\n",
    "        raise\n",
    "print(f\"Endpoint '{endpoint_name}' deleted using boto3.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905a0a4",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d904d",
   "metadata": {},
   "source": [
    "Now that you've successfully trained and evaluated a model in the cloud, you‚Äôve completed your first end-to-end managed ML workflow using SageMaker.\n",
    "\n",
    "---\n",
    "\n",
    "### Cleanup (Don‚Äôt Leave the Lights On)\n",
    "\n",
    "To avoid unnecessary AWS charges, we‚Äôll delete all the resources we created:\n",
    "\n",
    "- **CloudFormation Stack**:  \n",
    "  This will automatically delete:\n",
    "  - The SageMaker Studio domain\n",
    "  - The execution role\n",
    "  - The S3 bucket  \n",
    "  You can do this programmatically using `boto3` (recommended), or‚Ä¶  \n",
    "  *if you absolutely must*... use the [CloudFormation Console](https://console.aws.amazon.com/cloudformation) like a savage.\n",
    "\n",
    "- **CloudWatch Logs**:  \n",
    "  These are optional to clean up, but can be removed from the [CloudWatch Console](https://console.aws.amazon.com/cloudwatch) if desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea238cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All objects in bucket 'caip02-ml-bucket-zali' deleted.\n",
      "üß® Deleting stack: caip02-cloud-ml-stack\n",
      "‚è≥ Waiting for stack to be fully deleted...\n",
      "‚úÖ Stack 'caip02-cloud-ml-stack' successfully deleted.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "def delete_stack_and_wait(stack_name):\n",
    "    print(f\"üß® Deleting stack: {stack_name}\")\n",
    "    \n",
    "    # Initiate deletion\n",
    "    cf.delete_stack(StackName=stack_name)\n",
    "\n",
    "    # Wait until stack deletion is complete\n",
    "    waiter = cf.get_waiter(\"stack_delete_complete\")\n",
    "    print(\"‚è≥ Waiting for stack to be fully deleted...\")\n",
    "    waiter.wait(StackName=stack_name)\n",
    "\n",
    "    print(f\"‚úÖ Stack '{stack_name}' successfully deleted.\")\n",
    "\n",
    "cf = boto3.client(\"cloudformation\")\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = {o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs if o[\"OutputKey\"] == \"BucketName\"}[\"BucketName\"]\n",
    "\n",
    "# List and delete objects in batches of 1000\n",
    "while True:\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "    if 'Contents' not in response:\n",
    "        break\n",
    "\n",
    "    delete_keys = [{'Key': obj['Key']} for obj in response['Contents']]\n",
    "\n",
    "    s3.delete_objects(\n",
    "        Bucket=bucket_name,\n",
    "        Delete={'Objects': delete_keys}\n",
    "    )\n",
    "\n",
    "    if not response.get('IsTruncated'):\n",
    "        break\n",
    "\n",
    "print(f\"All objects in bucket '{bucket_name}' deleted.\")\n",
    "\n",
    "# Call it\n",
    "delete_stack_and_wait(stack_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b829aa-5d6a-41d3-a730-3c02e1afb1ea",
   "metadata": {},
   "source": [
    "## Wrap-Up & Takeaways "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda14a9a-0efd-4b36-bcd3-6275f515ada5",
   "metadata": {},
   "source": [
    "In this notebook, you built and deployed your first real-time ML inference service using Amazon SageMaker. You trained a model, deployed it to a secure endpoint, sent a live prediction request, and reviewed logs ‚Äî all using scalable, cloud-native tools.\n",
    "\n",
    "---\n",
    "\n",
    "This workflow reflects what real ML teams do in production:\n",
    "\n",
    "- Train models offline and save versioned artifacts to cloud storage  \n",
    "- Deploy models to managed endpoints with autoscaling and monitoring  \n",
    "- Invoke models from client apps or internal APIs using standardized formats  \n",
    "- Monitor latency, errors, and prediction volume with CloudWatch  \n",
    "- Tear down unused endpoints to control cost and surface stale deployments  \n",
    "- Use repeatable infrastructure (e.g., scripts, IaC, automation) instead of manual clicking\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
