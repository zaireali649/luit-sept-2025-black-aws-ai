# AWS Certified AI Practitioner - September 2025 Cohort

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![AWS](https://img.shields.io/badge/AWS-AI%20Practitioner-FF9900.svg)](https://aws.amazon.com/certification/certified-ai-practitioner/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **A comprehensive learning repository for AWS Certified AI Practitioner certification preparation, focusing on AI/ML fundamentals, hands-on implementation, and AWS AI services.**

## üìã Table of Contents

- [Overview](#overview)
- [Learning Objectives](#learning-objectives)
- [Prerequisites](#prerequisites)
- [Environment Setup](#environment-setup)
- [Project Structure](#project-structure)
- [Learning Path](#learning-path)
- [Week 1 ML Fundamentals & Assessment](#week-1-ml-fundamentals--assessment)
- [Week 2 MLOps & DevOps Concepts](#week-2-mlops--devops-concepts)
- [DevOps Best Practices](#devops-best-practices)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [Resources](#resources)

## üéØ Overview

This repository contains hands-on learning materials for the AWS Certified AI Practitioner certification. The content is structured as a progressive learning journey covering fundamental AI/ML concepts, practical implementations using Python and scikit-learn, and AWS AI service integration.

### Key Focus Areas
- **Artificial Intelligence Fundamentals**: Understanding AI, ML, DL, and GenAI
- **Machine Learning Lifecycle**: End-to-end ML model development
- **AWS AI Services**: Hands-on experience with AWS AI/ML tools
- **DevOps for ML**: Best practices for ML model deployment and maintenance

## üéì Learning Objectives

### Completed Learning Outcomes ‚úÖ

After completing the first three sessions, you will be able to:

- **Understand AI/ML Fundamentals**: Differentiate between AI, ML, Deep Learning, and Generative AI
- **Classification Concepts**: Distinguish between supervised vs unsupervised learning, and classification vs regression
- **Model Training**: Execute the complete ML lifecycle from data preparation to model evaluation
- **Evaluation Metrics**: Explain and compute key metrics including accuracy, precision, recall, and F1 score
- **Confusion Matrix**: Interpret confusion matrices and understand their practical implications
- **Real-world Applications**: Relate evaluation metrics to business decision-making scenarios
- **Hands-on Skills**: Build, train, and evaluate ML models using scikit-learn
- **Data Analysis**: Work with real datasets (Iris, Titanic) and perform exploratory data analysis
- **Model Interpretation**: Understand overfitting, underfitting, and feature importance
- **Cloud ML Training**: Deploy and manage ML training jobs using Amazon SageMaker
- **AWS Infrastructure**: Set up SageMaker Studio domains using CloudFormation templates
- **S3 Integration**: Upload datasets and retrieve model artifacts from S3 buckets
- **Managed Training**: Use SKLearn Estimator for scalable, reproducible model training
- **Cloud Monitoring**: Monitor training jobs through CloudWatch logs and metrics
- **Secure ML Workflows**: Understand IAM roles and permissions for ML operations
- **Model Deployment**: Deploy trained models to real-time SageMaker endpoints
- **Inference Types**: Understand real-time, batch, and async inference options
- **Custom Inference Scripts**: Create custom inference.py scripts for data preprocessing
- **Endpoint Management**: Deploy, test, and delete SageMaker endpoints programmatically
- **Real-time Predictions**: Send test payloads and interpret prediction results
- **Production Monitoring**: Access and review endpoint logs and metrics in CloudWatch
- **Error Handling**: Identify common inference errors and performance metrics
- **Cost Management**: Delete endpoints to avoid unnecessary charges

### Future Learning Goals üéØ

Additional objectives will be added as the course progresses, including:
- Advanced ML techniques and algorithms
- Additional AWS AI services integration
- MLOps and deployment practices
- Deep learning and neural networks
- Model deployment and inference endpoints

## üìö Prerequisites

### Technical Requirements
- **Python**: 3.11 or higher
- **Operating System**: Windows 10/11, macOS, or Linux
- **Memory**: Minimum 8GB RAM (16GB recommended)
- **Storage**: 5GB free space

### Knowledge Prerequisites
- Basic programming experience (preferably Python)
- Understanding of basic statistics and mathematics
- Familiarity with command line/terminal operations
- Basic understanding of cloud computing concepts

## üõ†Ô∏è Environment Setup

### 1. Clone the Repository
```bash
git clone https://github.com/your-org/luit-sept-2025-black-aws-ai.git
cd luit-sept-2025-black-aws-ai
```

### 2. Python Virtual Environment Setup

#### Option A: Using venv (Recommended)
```bash
# Create virtual environment
python -m venv .venv

# Activate virtual environment
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate
```

#### Option B: Using conda
```bash
# Create conda environment
conda create -n aws-ai-practitioner python=3.11
conda activate aws-ai-practitioner
```

### 3. Install Dependencies
```bash
# Install core dependencies
pip install -r requirements.txt

# Install Jupyter extensions (optional)
pip install jupyter_contrib_nbextensions
jupyter contrib nbextension install --user
```

### 4. Launch Jupyter Notebook
```bash
jupyter notebook
```

### 5. Verify Installation
Run the first cell in any notebook to verify all dependencies are correctly installed.

## üìÅ Project Structure

```
luit-sept-2025-black-aws-ai/
‚îú‚îÄ‚îÄ README.md                          # Project documentation
‚îú‚îÄ‚îÄ requirements.txt                   # Python dependencies
‚îú‚îÄ‚îÄ .gitignore                        # Git ignore rules
‚îú‚îÄ‚îÄ .github/                          # GitHub templates and workflows
‚îÇ   ‚îî‚îÄ‚îÄ pull_request_template.md     # PR template
‚îú‚îÄ‚îÄ caip_01_01/                       # Week 1, Call 1 materials
‚îÇ   ‚îú‚îÄ‚îÄ caip_week1_call1.ipynb       # Introduction to AI/ML fundamentals
‚îÇ   ‚îú‚îÄ‚îÄ *.png                        # Supporting images and diagrams
‚îÇ   ‚îî‚îÄ‚îÄ caip_week1_call1_intro_to_ai_ml.pdf  # Reference materials
‚îú‚îÄ‚îÄ caip_01_02/                       # Week 1, Call 2 materials
‚îÇ   ‚îú‚îÄ‚îÄ caip_week1_call2.ipynb       # Model evaluation and metrics
‚îÇ   ‚îú‚îÄ‚îÄ caip01_week1_call2.pdf       # Reference materials
‚îÇ   ‚îú‚îÄ‚îÄ train.csv                    # Titanic training dataset
‚îÇ   ‚îî‚îÄ‚îÄ test.csv                     # Titanic test dataset
‚îú‚îÄ‚îÄ caip_02_01/                       # Week 2, Call 1 materials
    ‚îú‚îÄ‚îÄ caip_week2_call1.ipynb       # Train and evaluate in the cloud
    ‚îú‚îÄ‚îÄ caip_week2_call1.pdf         # Reference materials
    ‚îú‚îÄ‚îÄ cf_templates/                # CloudFormation templates
    ‚îÇ   ‚îî‚îÄ‚îÄ sagemaker_infra.yaml     # SageMaker infrastructure setup
    ‚îú‚îÄ‚îÄ train_model.py               # Training script for SageMaker
    ‚îú‚îÄ‚îÄ requirements.in              # Training script dependencies
    ‚îú‚îÄ‚îÄ cleaned_titanic.csv          # Preprocessed dataset
    ‚îú‚îÄ‚îÄ train.csv                    # Original training data
    ‚îú‚îÄ‚îÄ test.csv                     # Original test data
    ‚îî‚îÄ‚îÄ *.png                        # Supporting images and diagrams
‚îî‚îÄ‚îÄ caip_02_02/                       # Week 2, Call 2 materials
    ‚îú‚îÄ‚îÄ caip_week2_call2.ipynb       # Deploy and test ML endpoints
    ‚îú‚îÄ‚îÄ caip_week2_call2.pdf         # Reference materials
    ‚îú‚îÄ‚îÄ cf_templates/                # CloudFormation templates
    ‚îÇ   ‚îî‚îÄ‚îÄ sagemaker_infra.yaml     # SageMaker infrastructure setup
    ‚îú‚îÄ‚îÄ inference.py                 # Custom inference script for endpoints
    ‚îú‚îÄ‚îÄ cleaned_titanic.csv          # Preprocessed dataset
    ‚îú‚îÄ‚îÄ inference_pipeline.png       # Real-time inference architecture diagram
    ‚îú‚îÄ‚îÄ sagemaker_endpoint_logs.png  # CloudWatch logs screenshot
    ‚îî‚îÄ‚îÄ sagemaker_metrics.png        # CloudWatch metrics screenshot
```

## üó∫Ô∏è Learning Path

### Week 1: AI/ML Fundamentals

#### Call 1: Introduction to AI, ML, DL, and GenAI ‚úÖ *Completed*
- üìì [`caip_01_01/caip_week1_call1.ipynb`](caip_01_01/caip_week1_call1.ipynb)
- **Topics Covered:**
  - Differences between AI, ML, Deep Learning, and Generative AI
  - Supervised vs Unsupervised Learning (with practical examples)
  - Classification vs Regression problems
  - Overfitting vs Underfitting concepts
  - Feature importance analysis
  - Complete ML lifecycle: Prepare ‚Üí Train ‚Üí Predict & Evaluate
- **Hands-on Experience:**
  - Built classification models using scikit-learn
  - Worked with the Iris dataset
  - Created decision trees and evaluated model performance
  - Visualized data patterns and model decision boundaries

#### Call 2: Model Evaluation - "How Good Is Your Model?" ‚úÖ *Completed*
- üìì [`caip_01_02/caip_week1_call2.ipynb`](caip_01_02/caip_week1_call2.ipynb)
- **Topics Covered:**
  - Introduction to Kaggle and the Titanic dataset
  - Key evaluation metrics: Accuracy, Precision, Recall, F1 Score
  - Understanding and interpreting confusion matrices
  - Why accuracy alone can be misleading
  - Real-world application of metrics (Facebook Marketplace gun detection example)
  - Trade-offs between different types of errors (false positives vs false negatives)
- **Hands-on Experience:**
  - Loaded and preprocessed the Titanic dataset
  - Built and evaluated classification models
  - Computed and visualized evaluation metrics
  - Interpreted classification reports and confusion matrices

### Week 2: Cloud-Based ML with AWS SageMaker

#### Call 1: Train and Evaluate in the Cloud ‚úÖ *Completed*
- üìì [`caip_02_01/caip_week2_call1.ipynb`](caip_02_01/caip_week2_call1.ipynb)
- **Topics Covered:**
  - Benefits of cloud-based ML training (scalability, reproducibility, separation of concerns)
  - Infrastructure setup using CloudFormation templates
  - SageMaker Studio domain and IAM role configuration
  - S3 integration for data storage and model artifacts
  - SKLearn Estimator for managed training jobs
  - CloudWatch logging and monitoring
  - Secure ML workflows with proper permissions
- **Hands-on Experience:**
  - Deployed SageMaker infrastructure programmatically
  - Uploaded datasets to S3 buckets
  - Created and executed training scripts for SageMaker
  - Launched managed training jobs using SKLearn Estimator
  - Monitored training progress through CloudWatch logs
  - Retrieved and analyzed model artifacts from S3

#### Call 2: Deploy and Test Your First ML Endpoint ‚úÖ *Completed*
- üìì [`caip_02_02/caip_week2_call2.ipynb`](caip_02_02/caip_week2_call2.ipynb)
- **Topics Covered:**
  - Real-time, batch, and async inference options in SageMaker
  - Model artifact location and loading from S3
  - Custom inference scripts for data preprocessing
  - SKLearnModel deployment to real-time endpoints
  - Endpoint invocation using boto3 and JSON payloads
  - CloudWatch logs and metrics for endpoint monitoring
  - Common inference errors and performance metrics
  - Cost management through endpoint deletion
- **Hands-on Experience:**
  - Located and loaded trained model artifacts from S3
  - Created custom inference.py script with model_fn and predict_fn
  - Deployed scikit-learn model to real-time SageMaker endpoint
  - Sent test payloads and interpreted prediction results
  - Monitored endpoint performance through CloudWatch logs and metrics
  - Identified and understood common inference errors
  - Deleted endpoints to avoid unnecessary costs

### Future Sessions
*Content will be added as sessions are completed*

## üéì Week 1 ML Fundamentals & Assessment

### Overview of ML Fundamentals Implementation

Week 1 established the foundational knowledge for machine learning through hands-on implementation with scikit-learn. The sessions demonstrated how to transition from theoretical AI/ML concepts to practical model building and evaluation.

### AI/ML Concepts Coverage

**What We Implemented:**
- **AI/ML/DL/GenAI Distinctions**: Clear definitions with real-world examples (email management system)
- **Learning Types**: Supervised vs unsupervised learning with visual demonstrations
- **Problem Types**: Classification vs regression with practical examples
- **Model Behavior**: Underfitting vs overfitting with decision tree visualizations
- **Data Challenges**: Class imbalance concepts and business implications

**Key Learning Outcomes:**
- Understanding the complete ML lifecycle (Prepare ‚Üí Train ‚Üí Predict & Evaluate)
- Hands-on experience with real datasets (Iris, Titanic)
- Model evaluation and interpretation skills
- Business context for ML decisions

### ML Lifecycle Implementation

#### 1. **Data Preparation & Exploration**
```python
# Data Loading & Preprocessing Pipeline
‚îú‚îÄ‚îÄ Dataset Loading (Iris, Titanic)
‚îú‚îÄ‚îÄ Data Cleaning (handle missing values, encode categories)
‚îú‚îÄ‚îÄ Feature Selection (relevant columns)
‚îú‚îÄ‚îÄ Train/Test Split (80/20 with stratification)
‚îî‚îÄ‚îÄ Feature Scaling (StandardScaler for consistency)
```

**Covered Techniques:**
- **Data Loading**: CSV files and sklearn datasets
- **Basic Cleaning**: Drop missing values, categorical encoding
- **Feature Engineering**: Simple feature selection
- **Data Splitting**: Train/test split with random state for reproducibility

#### 2. **Model Training & Selection**
```python
# Model Training Pipeline
‚îú‚îÄ‚îÄ Decision Tree Classifier (baseline model)
‚îú‚îÄ‚îÄ Random Forest Classifier (ensemble method)
‚îú‚îÄ‚îÄ Feature Importance Analysis
‚îî‚îÄ‚îÄ Model Visualization (decision tree plots)
```

**Models Implemented:**
- **Decision Trees**: Interpretable baseline with visual decision boundaries
- **Random Forest**: Ensemble method for improved performance
- **Feature Importance**: Understanding which features drive predictions

#### 3. **Model Evaluation & Interpretation**
```python
# Evaluation Metrics Pipeline
‚îú‚îÄ‚îÄ Accuracy (overall performance)
‚îú‚îÄ‚îÄ Precision (false positive control)
‚îú‚îÄ‚îÄ Recall (false negative control)
‚îú‚îÄ‚îÄ F1 Score (balanced metric)
‚îú‚îÄ‚îÄ Confusion Matrix (raw and normalized)
‚îî‚îÄ‚îÄ Classification Report (comprehensive metrics)
```

**Evaluation Techniques:**
- **Core Metrics**: Accuracy, precision, recall, F1 score
- **Confusion Matrix**: Both raw counts and normalized proportions
- **Business Context**: Real-world implications (Facebook Marketplace gun detection)
- **Model Comparison**: Decision Tree vs Random Forest performance

### Real-World Application Examples

#### Business Context Integration
- **Titanic Survival Prediction**: Binary classification with imbalanced data
- **Facebook Marketplace Gun Detection**: Real-world ML system with business implications
- **Loan Approval System**: Supervised learning with feature importance
- **Customer Segmentation**: Unsupervised learning with clustering

#### Error Analysis & Business Impact
| Error Type | Business Impact | Example Context |
|------------|----------------|-----------------|
| **False Positive** | User trust issues | Flagging innocent marketplace items |
| **False Negative** | Safety/compliance risk | Missing actual gun listings |
| **Class Imbalance** | Misleading accuracy | 95% accuracy with 0% fraud detection |

### Production Readiness Gaps & Future Enhancements

#### Current Implementation (ML Fundamentals)
‚úÖ **Completed:**
- Basic ML lifecycle understanding
- Core evaluation metrics
- Model training and prediction
- Business context integration
- Hands-on dataset experience

#### Production Enhancements Needed:
üîß **Advanced Data Preprocessing:**
- **Comprehensive data cleaning techniques**: Data profiling, automated quality analysis, schema validation
- **Advanced missing value handling**: Imputation (mean/median/mode), interpolation, KNN imputation, multiple imputation
- **Feature engineering and creation**: Combining features, interaction terms, polynomial features, domain-specific transformations
- **Data validation and quality checks**: Range validation, business rule checks, statistical anomaly detection
- **Outlier detection and treatment**: IQR method, Z-score analysis, Isolation Forest, domain-specific outlier rules

üîß **Robust Model Evaluation:**
- **Cross-validation for model selection**: K-fold CV, stratified CV, time series CV for robust performance estimates
- **ROC curves and AUC metrics**: True Positive Rate vs False Positive Rate plots, Area Under Curve for binary classification
- **Precision-Recall curves**: Better for imbalanced data, shows precision vs recall at different thresholds
- **Learning curves for overfitting detection**: Training vs validation performance plots to identify overfitting
- **Stratified sampling for imbalanced datasets**: Maintains class distribution across train/test splits

üîß **Model Selection & Tuning:**
- **Hyperparameter tuning**: Grid Search (exhaustive), Random Search (efficient), Bayesian Optimization (guided search)
- **Model comparison methodology**: Statistical tests (t-tests, Wilcoxon), ensemble methods, performance benchmarking
- **Train/validation/test split strategy**: Proper data splitting to prevent data leakage and overfitting
- **Cross-validation techniques**: Multiple validation strategies for robust model selection
- **Performance analysis by subgroups**: Model performance breakdown by demographic or feature groups

üîß **Advanced Analytics:**
- **Comprehensive exploratory data analysis (EDA)**: Automated data profiling, statistical summaries, distribution analysis
- **Correlation analysis and heatmaps**: Pearson/Spearman correlations, feature relationship visualization
- **Distribution analysis and statistical summaries**: Histograms, box plots, violin plots, descriptive statistics
- **Feature selection techniques**: Correlation analysis, mutual information, recursive feature elimination, LASSO
- **Dimensionality reduction concepts**: PCA (linear), t-SNE (non-linear), UMAP (modern alternative) for visualization

üîß **Model Interpretability:**
- **SHAP values or LIME for model explanation**: SHAP (Shapley values from game theory), LIME (local approximation) for individual predictions
- **Partial dependence plots**: Shows feature effects while averaging over other features, reveals interactions
- **Global vs local interpretability**: Global (overall model behavior) vs Local (individual prediction explanations)
- **Model-agnostic interpretation techniques**: Works with any ML model, not specific to algorithm type
- **Error pattern analysis**: Analysis of misclassified examples, identifying systematic model failures

üîß **Production Considerations:**
- **Model persistence and loading**: Serialization (pickle, joblib), model registry, version control for trained models
- **Data pipeline concepts**: ETL/ELT processes, data validation, feature stores for centralized feature management
- **Model versioning basics**: Tracking model versions, performance comparison, rollback strategies
- **A/B testing concepts for models**: Champion/challenger testing, statistical significance, business impact measurement
- **Performance monitoring**: Data drift detection, concept drift, model decay monitoring, alerting systems

### ML Fundamentals Lessons Learned

#### What Worked Well:
1. **Progressive Learning**: Clear progression from theory to practice
2. **Real-World Context**: Business examples made concepts tangible
3. **Visual Learning**: Decision tree plots and confusion matrices
4. **Hands-On Experience**: Multiple datasets and model types
5. **Business Integration**: Understanding error types and their impact

#### Areas for Improvement:
1. **Data Quality**: Need more robust preprocessing techniques
2. **Model Selection**: Missing systematic comparison methodology
3. **Evaluation Depth**: Need advanced metrics and validation techniques
4. **Feature Engineering**: Limited feature creation and selection
5. **Production Readiness**: Missing deployment and monitoring concepts

### Next Steps for Advanced ML

1. **Implement Cross-Validation** for robust model evaluation (K-fold, stratified sampling)
2. **Add Advanced Metrics** (ROC/AUC curves for binary classification, Precision-Recall curves for imbalanced data)
3. **Include Feature Engineering** techniques (feature creation, selection, transformation) and selection methods
4. **Demonstrate Hyperparameter Tuning** with grid search, random search, and Bayesian optimization
5. **Add Model Interpretability** tools (SHAP for feature attribution, LIME for local explanations)
6. **Build Production Pipelines** with model persistence (pickle/joblib), versioning, and monitoring

### Curriculum Progression Recommendations

#### Week 1 Enhancement (if revisiting):
- **Comprehensive EDA**: Pandas profiling, automated data quality analysis, statistical summaries
- **Cross-Validation**: K-fold validation, stratified sampling for robust performance estimates
- **Advanced Metrics**: ROC curves and AUC for binary classification, Precision-Recall curves for imbalanced data
- **Feature Engineering**: Feature creation, selection techniques, domain-specific transformations
- **Hyperparameter Tuning**: Grid search, random search, Bayesian optimization for model improvement

This foundation provides essential ML knowledge with clear paths for advanced learning and production implementation.

## üèóÔ∏è Week 2 MLOps & DevOps Concepts

### Overview of MLOps Implementation

Week 2 introduced fundamental MLOps (Machine Learning Operations) concepts through hands-on implementation with Amazon SageMaker. The sessions demonstrated how to transition from local ML development to production-ready cloud-based ML workflows.

### Infrastructure as Code (IaC) with CloudFormation

**What We Implemented:**
- **Automated Infrastructure Provisioning**: Used CloudFormation templates to create SageMaker Studio domains, IAM roles, and S3 buckets
- **Programmatic Deployment**: Deployed infrastructure using `boto3` instead of manual AWS Console operations
- **Reproducible Environments**: Ensured consistent infrastructure across different environments

**Key Files:**
- [`caip_02_01/cf_templates/sagemaker_infra.yaml`](caip_02_01/cf_templates/sagemaker_infra.yaml) - Complete infrastructure template
- Infrastructure includes: SageMaker Studio Domain, IAM Execution Role, S3 Bucket, User Profile

**DevOps Benefits:**
- **Version Control**: Infrastructure changes tracked in Git
- **Automation**: Eliminates manual setup errors
- **Scalability**: Easy to replicate across regions/accounts
- **Cost Control**: Automated cleanup prevents resource sprawl

### MLOps Pipeline Components

#### 1. **Data Management & Storage**
```yaml
Data Flow:
Local CSV ‚Üí S3 Bucket ‚Üí SageMaker Training Job ‚Üí Model Artifacts ‚Üí S3 Storage
```
- **S3 Integration**: Centralized data storage for training datasets
- **Artifact Management**: Automatic model artifact storage and versioning
- **Data Preprocessing**: Consistent data cleaning and feature engineering

#### 2. **Training Pipeline**
```python
# Training Script Structure
train_model.py:
‚îú‚îÄ‚îÄ Data Loading (from S3)
‚îú‚îÄ‚îÄ Feature Engineering
‚îú‚îÄ‚îÄ Model Training (Decision Tree)
‚îú‚îÄ‚îÄ Model Evaluation (Metrics & Logging)
‚îî‚îÄ‚îÄ Model Persistence (joblib serialization)
```

**Key Features:**
- **Containerized Training**: SKLearn Estimator with pre-built containers
- **Scalable Compute**: Choose instance types based on dataset size
- **Reproducible Results**: Fixed random seeds and consistent environments
- **CloudWatch Logging**: Centralized logging for debugging and monitoring

#### 3. **Model Deployment & Inference**
```python
# Inference Pipeline
inference.py:
‚îú‚îÄ‚îÄ model_fn() - Load model and scaler
‚îú‚îÄ‚îÄ predict_fn() - Preprocess input and predict
‚îî‚îÄ‚îÄ CloudWatch Logging - Monitor predictions
```

**Deployment Options Covered:**
- **Real-time Endpoints**: Low-latency predictions via HTTPS API
- **Custom Inference Scripts**: Data preprocessing at inference time
- **Model Versioning**: Track different model versions in S3

### Monitoring & Observability

#### CloudWatch Integration
- **Training Job Logs**: Monitor training progress and errors
- **Endpoint Logs**: Track inference requests and responses
- **Performance Metrics**: Model latency, error rates, invocation counts
- **Cost Monitoring**: Track resource usage and costs

#### Key Metrics Tracked:
| Metric | Purpose | Action Threshold |
|--------|---------|------------------|
| `ModelLatency` | Response time | > 1000ms |
| `Invocation4XXErrors` | Client errors | > 5% |
| `Invocation5XXErrors` | Server errors | > 1% |
| `MemoryUtilization` | Resource usage | > 80% |

### Security & Compliance

#### IAM Best Practices
- **Least Privilege Access**: SageMaker execution role with minimal required permissions
- **S3 Bucket Policies**: Restricted access to training data and model artifacts
- **VPC Configuration**: Network isolation for SageMaker Studio

#### Security Features Implemented:
```yaml
IAM Permissions:
‚îú‚îÄ‚îÄ S3: GetObject, PutObject, ListBucket (specific bucket only)
‚îú‚îÄ‚îÄ CloudWatch: CreateLogGroup, PutLogEvents
‚îî‚îÄ‚îÄ SageMaker: Full access for training and deployment
```

### Cost Management & Optimization

#### Resource Cleanup
- **Automated Cleanup**: Programmatic deletion of CloudFormation stacks
- **Endpoint Management**: Delete idle endpoints to avoid charges
- **S3 Object Cleanup**: Remove temporary artifacts and datasets

#### Cost Optimization Strategies:
- **Right-sizing Instances**: Use appropriate instance types for training
- **Spot Instances**: (Future enhancement) Use spot instances for training jobs
- **Auto-scaling**: (Future enhancement) Scale endpoints based on demand

### Production Readiness Gaps & Future Enhancements

#### Current Implementation (Proof of Concept)
‚úÖ **Completed:**
- Basic MLOps pipeline (train ‚Üí deploy ‚Üí monitor)
- Infrastructure as Code
- CloudWatch monitoring
- Automated cleanup

#### Production Enhancements Needed:
üîß **Model Management:**
- **Model versioning and registry (MLflow integration)**: Centralized model storage, metadata tracking, experiment management
- **A/B testing capabilities**: Champion/challenger model comparison, gradual rollout, statistical significance testing
- **Model performance monitoring and drift detection**: Real-time performance tracking, data drift alerts, model decay detection

üîß **CI/CD Pipeline:**
- **Automated testing of training scripts**: Unit tests, integration tests, data validation tests
- **Model validation tests (accuracy thresholds)**: Performance gates, model quality checks, business metric validation
- **Automated deployment pipeline**: Blue-green deployment, canary releases, automated model promotion
- **Rollback strategies**: Automated reversion to previous model versions, circuit breakers, health checks

üîß **Advanced Monitoring:**
- **Custom CloudWatch dashboards**: Business and technical metrics visualization, real-time model performance tracking
- **Alerting on model degradation**: Automated alerts for performance drops, error rate increases, latency spikes
- **Data drift monitoring**: Statistical tests to detect input data distribution changes over time
- **Business metrics tracking**: Revenue impact, user engagement, conversion rates affected by model predictions

üîß **Security Hardening:**
- **VPC endpoints for S3 (data in transit encryption)**: Private connectivity to AWS services, encrypted data transmission
- **S3 bucket encryption at rest**: Server-side encryption (SSE-S3, SSE-KMS) for stored data protection
- **Secrets management for API keys**: AWS Secrets Manager, IAM roles, environment variable security
- **Network security groups**: Firewall rules, network access control, subnet isolation

üîß **Scalability:**
- **Auto-scaling configuration for endpoints**: Dynamic scaling based on traffic, cost optimization, performance maintenance
- **Batch inference pipeline**: Large-scale predictions on S3 data, cost-effective for non-real-time use cases
- **Multi-model endpoints**: Multiple models on single endpoint, traffic splitting, model routing
- **Load testing capabilities**: Performance testing, capacity planning, stress testing for production readiness

### DevOps Lessons Learned

#### What Worked Well:
1. **Infrastructure as Code**: CloudFormation templates made setup reproducible
2. **Separation of Concerns**: Training scripts separate from infrastructure
3. **Centralized Logging**: CloudWatch provided excellent visibility
4. **Automated Cleanup**: Prevented cost overruns

#### Areas for Improvement:
1. **Error Handling**: Need retry logic and circuit breakers
2. **Testing**: Missing unit tests for training and inference scripts
3. **Documentation**: Need API documentation for deployed endpoints
4. **Monitoring**: Need custom dashboards and alerting

### Next Steps for Production MLOps

1. **Implement MLflow** for experiment tracking and model registry (centralized model management, metadata tracking)
2. **Add CI/CD Pipeline** with GitHub Actions for automated testing/deployment (unit tests, integration tests, automated model promotion)
3. **Create Custom Dashboards** for business and technical metrics (CloudWatch dashboards, real-time monitoring)
4. **Implement Data Validation** using Great Expectations (data quality checks, schema validation, anomaly detection)
5. **Add Security Hardening** with encryption and VPC endpoints (data in transit/at rest encryption, network isolation)
6. **Build Batch Inference Pipeline** for large-scale predictions (S3-based batch processing, cost-effective inference)

This foundation provides a solid starting point for understanding MLOps principles, with clear paths for scaling to production-ready systems.


## üîß DevOps Best Practices

### Version Control
- **Git Workflow**: Feature branches, pull requests, and code reviews
- **Notebook Management**: Use `nbstripout` to remove outputs before committing
- **Model Versioning**: Track model artifacts using DVC or MLflow

### Environment Management
```bash
# Always use virtual environments
python -m venv .venv
source .venv/bin/activate  # or .venv\Scripts\activate on Windows

# Pin dependency versions
pip freeze > requirements.txt

# Use environment variables for configuration
cp .env.example .env
# Edit .env with your specific settings
```

### Code Quality
```bash
# Install development dependencies
pip install black flake8 pytest jupyter-nbextensions-configurator

# Format code
black scripts/ tests/

# Lint code
flake8 scripts/ tests/

# Run tests
pytest tests/
```

### Jupyter Notebook Best Practices
- Clear outputs before committing: `jupyter nbconvert --clear-output --inplace *.ipynb`
- Use meaningful cell markdown for documentation
- Keep notebooks focused on single topics
- Extract reusable code into Python modules

### ML Model Lifecycle
1. **Experimentation**: Use notebooks for exploration and prototyping
2. **Development**: Convert notebook code to Python modules
3. **Testing**: Unit tests for data processing and model functions
4. **Deployment**: Containerize models for consistent environments
5. **Monitoring**: Track model performance and data drift

## üö® Troubleshooting

### Common Issues

#### Jupyter Kernel Issues
```bash
# Reinstall kernel
python -m ipykernel install --user --name=aws-ai-practitioner
```

#### Package Installation Errors
```bash
# Upgrade pip
python -m pip install --upgrade pip

# Clear pip cache
pip cache purge

# Install with verbose output
pip install -v package-name
```

#### Memory Issues
- Reduce dataset size for initial learning
- Use `del` to free memory in notebooks
- Restart kernel regularly during development

#### AWS Credentials
```bash
# Configure AWS CLI
aws configure

# Or use environment variables
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret
export AWS_DEFAULT_REGION=us-east-1
```

## üìñ Resources

### Official AWS Resources
- [AWS Certified AI Practitioner Exam Guide](https://aws.amazon.com/certification/certified-ai-practitioner/)
- [AWS Machine Learning Training](https://aws.amazon.com/training/learning-paths/machine-learning/)
- [AWS AI Services Documentation](https://docs.aws.amazon.com/ai-services/)

### Additional Learning Materials
- [Scikit-learn Documentation](https://scikit-learn.org/stable/)
- [Jupyter Notebook Documentation](https://jupyter-notebook.readthedocs.io/)
- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)
- [AWS Machine Learning Blog](https://aws.amazon.com/blogs/machine-learning/)

### Community Resources
- [AWS Machine Learning Community](https://aws.amazon.com/developer/community/machine-learning/)
- [Kaggle Learn](https://www.kaggle.com/learn)
- [Papers with Code](https://paperswithcode.com/)

---

## üè∑Ô∏è Tags

`aws` `ai` `machine-learning` `certification` `python` `jupyter` `devops` `mlops` `education` `hands-on-learning`

---

**Happy Learning! üöÄ**

*For questions or support, please reach out to the course instructors or use the course discussion forums.*