{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763c0e9a-2387-493f-94bc-96ffecd4a9c8",
   "metadata": {},
   "source": [
    "## Certified AI Practitioner Week 04 Call 01 - Deploying Open Models the Right Way (From ü§ó to SageMaker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84b897-7cba-4d9e-920f-ca6193b7e4ad",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880bca35-6340-4db1-9b2b-760da3539d1f",
   "metadata": {},
   "source": [
    "- Understand what Hugging Face is and its role in modern GenAI workflows\n",
    "- Decide when to use Hugging Face models vs Amazon Bedrock\n",
    "- Deploy a Hugging Face model on SageMaker using the `HuggingFaceModel` class\n",
    "- Invoke a real-time text classification endpoint and interpret the results\n",
    "- Explain how SageMaker manages Hugging Face models using containers, IAM, and S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e27421e",
   "metadata": {},
   "source": [
    "## ü§ó What is Hugging Face?\n",
    "\n",
    "Hugging Face is one of the most important open-source communities in machine learning today.\n",
    "\n",
    "It provides:\n",
    "\n",
    "- **The Hub** ‚Äì A massive library of pre-trained models for tasks like sentiment analysis, translation, summarization, Q&A, image classification, audio transcription, and more.\n",
    "- **Transformers library** ‚Äì A Python library for using and fine-tuning state-of-the-art deep learning models from across research and industry.\n",
    "- **Datasets and Tokenizers** ‚Äì Tools to make preprocessing, tokenization, and evaluation easier across domains.\n",
    "- **Community Contributions** ‚Äì Most models on Hugging Face are shared by researchers, developers, and companies around the world.\n",
    "\n",
    "You can use Hugging Face models:\n",
    "- Locally (with Python)\n",
    "- Through APIs (hosted by Hugging Face)\n",
    "- Or in the cloud (via SageMaker, Vertex AI, etc.)\n",
    "\n",
    "In this notebook, we‚Äôll use Hugging Face models **on SageMaker** using a managed container, without needing to build a custom Docker image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8988f1",
   "metadata": {},
   "source": [
    "## ü§ó Why Hugging Face?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb994f",
   "metadata": {},
   "source": [
    "Hugging Face is an open-source platform and community centered around modern machine learning ‚Äî especially natural language processing (NLP).\n",
    "\n",
    "It provides:\n",
    "\n",
    "- A massive hub of pre-trained models covering text, vision, and audio tasks\n",
    "- Tools like `transformers`, `datasets`, and `tokenizers` for building with ML\n",
    "- A friendly ecosystem for researchers, engineers, and enterprises\n",
    "\n",
    "With Hugging Face:\n",
    "\n",
    "- ‚úÖ You can browse and deploy thousands of open models\n",
    "- ‚úÖ Fine-tune models for your data\n",
    "- ‚úÖ Run locally or in cloud platforms like SageMaker\n",
    "\n",
    "**Popular Tasks:**  \n",
    "Text Classification ¬∑ Summarization ¬∑ Question Answering ¬∑ Translation ¬∑ Embeddings ¬∑ Image Recognition ¬∑ Speech-to-Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d47b4",
   "metadata": {},
   "source": [
    "## When to Use Hugging Face vs Bedrock\n",
    "\n",
    "| Use Case                    | Bedrock                        | Hugging Face                      |\n",
    "|-----------------------------|--------------------------------|------------------------------------|\n",
    "| Managed security            | ‚úÖ Yes                         | ‚ùå You manage IAM + roles          |\n",
    "| Plug-and-play UX            | ‚úÖ Playground, no code         | ‚ùå Requires code and SDK           |\n",
    "| Bring your own model        | ‚ùå Not yet supported           | ‚úÖ Upload or fine-tune your own    |\n",
    "| Community models / open     | ‚ùå Limited to AWS partners     | ‚úÖ Full access to public models    |\n",
    "| CI/CD and customization     | üî∂ Limited                     | ‚úÖ Full flexibility                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6b15f",
   "metadata": {},
   "source": [
    "## Infrastructure Setup with CloudFormation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f1a9b6",
   "metadata": {},
   "source": [
    "To train models in SageMaker Studio, we first need to provision the necessary infrastructure.\n",
    "\n",
    "In this step, we‚Äôll use an automated **CloudFormation template** to create:\n",
    "\n",
    "- A **SageMaker Studio domain** for running cloud-based notebooks  \n",
    "- An **IAM execution role** with S3 and SageMaker permissions  \n",
    "- A dedicated **S3 bucket** to store training data and model artifacts  \n",
    "\n",
    "Instead of clicking through the AWS Console, we‚Äôll deploy this setup programmatically using `boto3`. The stack will output everything you need - including the bucket name and role ARN - to use in the next steps of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f658ff1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating stack: caip04-cloud-ml-stack\n",
      "‚è≥ Waiting for caip04-cloud-ml-stack to complete...\n",
      "‚úÖ Stack operation completed.\n",
      "üîß Stack Outputs:\n",
      "{\n",
      "  \"StudioUserName\": \"caip04-user\",\n",
      "  \"BucketName\": \"caip04-ml-bucket-zali\",\n",
      "  \"DomainId\": \"d-qs1spbknmac6\",\n",
      "  \"RoleArn\": \"arn:aws:iam::458806987020:role/caip04-execution-role-zali\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "def stack_exists(name):\n",
    "    try:\n",
    "        cf.describe_stacks(StackName=name)\n",
    "        return True\n",
    "    except cf.exceptions.ClientError as e:\n",
    "        if \"does not exist\" in str(e):\n",
    "            return False\n",
    "        raise  # re-raise any unexpected error\n",
    "\n",
    "def deploy_stack(stack_name, template_body, parameters):\n",
    "    if stack_exists(stack_name):\n",
    "        print(f\"üîÑ Updating stack: {stack_name}\")\n",
    "        try:\n",
    "            response = cf.update_stack(\n",
    "                StackName=stack_name,\n",
    "                TemplateBody=template_body,\n",
    "                Parameters=parameters,\n",
    "                Capabilities=[\"CAPABILITY_NAMED_IAM\"]\n",
    "            )\n",
    "            waiter = cf.get_waiter(\"stack_update_complete\")\n",
    "        except cf.exceptions.ClientError as e:\n",
    "            if \"No updates are to be performed\" in str(e):\n",
    "                print(\"‚úÖ No changes detected.\")\n",
    "                # Print outputs\n",
    "                outputs = cf.describe_stacks(StackName=stack_name)[\"Stacks\"][0][\"Outputs\"]\n",
    "                print(\"üîß Stack Outputs:\")\n",
    "                print(json.dumps({o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs}, indent=2))\n",
    "                return outputs\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        print(f\"üöÄ Creating stack: {stack_name}\")\n",
    "        response = cf.create_stack(\n",
    "            StackName=stack_name,\n",
    "            TemplateBody=template_body,\n",
    "            Parameters=parameters,\n",
    "            Capabilities=[\"CAPABILITY_NAMED_IAM\"]\n",
    "        )\n",
    "        waiter = cf.get_waiter(\"stack_create_complete\")\n",
    "\n",
    "    print(f\"‚è≥ Waiting for {stack_name} to complete...\")\n",
    "    waiter.wait(StackName=stack_name)\n",
    "    print(\"‚úÖ Stack operation completed.\")\n",
    "\n",
    "    # Print outputs\n",
    "    outputs = cf.describe_stacks(StackName=stack_name)[\"Stacks\"][0][\"Outputs\"]\n",
    "    print(\"üîß Stack Outputs:\")\n",
    "    print(json.dumps({o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs}, indent=2))\n",
    "    return outputs\n",
    "\n",
    "ec2 = boto3.client(\"ec2\")\n",
    "cf = boto3.client(\"cloudformation\")\n",
    "\n",
    "# Get the default VPC ID\n",
    "vpc_id = ec2.describe_vpcs(Filters=[{\"Name\": \"isDefault\", \"Values\": [\"true\"]}])[\"Vpcs\"][0][\"VpcId\"]\n",
    "\n",
    "# Get a public subnet ID in that VPC\n",
    "subnets = ec2.describe_subnets(Filters=[{\"Name\": \"vpc-id\", \"Values\": [vpc_id]}])\n",
    "subnet_id = subnets[\"Subnets\"][0][\"SubnetId\"]\n",
    "\n",
    "# Load your template\n",
    "with open(\"cf_templates/sagemaker_infra.yaml\") as f:\n",
    "    template_body = f.read()\n",
    "\n",
    "bucketNameSuffix = \"zali\"\n",
    "stack_name = \"caip04-cloud-ml-stack\"\n",
    "\n",
    "parameters = [\n",
    "    {\"ParameterKey\": \"BucketNameSuffix\", \"ParameterValue\": bucketNameSuffix},\n",
    "    {\"ParameterKey\": \"VpcId\", \"ParameterValue\": vpc_id},\n",
    "    {\"ParameterKey\": \"SubnetId\", \"ParameterValue\": subnet_id}\n",
    "]\n",
    "\n",
    "outputs = deploy_stack(stack_name, template_body, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28fe64",
   "metadata": {},
   "source": [
    "## The Hugging Face Ecosystem: Models, Datasets, and Spaces\n",
    "\n",
    "When you visit [huggingface.co](https://huggingface.co), you'll see three main categories at the top:\n",
    "\n",
    "---\n",
    "\n",
    "### Models\n",
    "This is the **model hub** ‚Äî where you'll find thousands of pre-trained and fine-tuned machine learning models.\n",
    "\n",
    "You can filter by:\n",
    "- Task (e.g., sentiment analysis, summarization, translation)\n",
    "- Framework (e.g., PyTorch, TensorFlow)\n",
    "- Language or modality (text, image, audio)\n",
    "\n",
    "In this notebook, we'll use one of these models:  \n",
    "`distilbert-base-uncased-finetuned-sst-2-english`\n",
    "\n",
    "---\n",
    "\n",
    "### Datasets\n",
    "The **Datasets** tab contains hundreds of benchmark and real-world datasets used for training, testing, and evaluation.\n",
    "\n",
    "You‚Äôll find:\n",
    "- Benchmark datasets like SST-2, SQuAD, IMDB\n",
    "- Real-world data for tasks like classification, translation, QA, etc.\n",
    "- Tools for loading and preprocessing directly with `datasets` library\n",
    "\n",
    "---\n",
    "\n",
    "### Spaces\n",
    "Spaces are **interactive AI web apps** ‚Äî powered by models from the hub.\n",
    "\n",
    "- Built using Gradio or Streamlit\n",
    "- Visual interface for testing models without code\n",
    "- Used by companies and individuals to showcase ML apps\n",
    "\n",
    "‚úÖ Spaces make models **interactive**  \n",
    "‚úÖ Datasets make models **trainable**  \n",
    "‚úÖ Models make ML **reusable**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b1b9e",
   "metadata": {},
   "source": [
    "\n",
    "## What Is the Transformers Library?\n",
    "\n",
    "The `transformers` library by Hugging Face is the most widely used Python package for working with **pretrained deep learning models** ‚Äî especially **transformer-based architectures** like BERT, GPT, T5, RoBERTa, DistilBERT, and more.\n",
    "\n",
    "It provides:\n",
    "\n",
    "- ‚úÖ Simple APIs for loading models, tokenizers, and pipelines\n",
    "- ‚úÖ Access to thousands of models on the Hugging Face Hub\n",
    "- ‚úÖ Compatibility with PyTorch, TensorFlow, and JAX\n",
    "- ‚úÖ Built-in support for tasks like:\n",
    "  - Text classification\n",
    "  - Summarization\n",
    "  - Question answering\n",
    "  - Translation\n",
    "  - Text generation\n",
    "  - Zero-shot classification\n",
    "  - Token classification (NER)\n",
    "  - Image and audio tasks (via multi-modal models)\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Sentiment Analysis in One Line\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this course!\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "> Output:\n",
    "```python\n",
    "[{'label': 'POSITIVE', 'score': 0.999}]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "- You don‚Äôt need to train or fine-tune anything to get started.\n",
    "- Pretrained models + pipelines = instant value from research-grade ML.\n",
    "- The same models used in academic papers, production systems, and Hugging Face Spaces are accessible in just a few lines of code.\n",
    "\n",
    "---\n",
    "\n",
    "For this lesson, we‚Äôll let SageMaker **host one of these models for us**, using the `transformers` library under the hood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86af3f",
   "metadata": {},
   "source": [
    "## Example Model: distilbert-base-uncased-finetuned-sst-2-english\n",
    "\n",
    "This is a lightweight, pre-trained **DistilBERT** model fine-tuned on the SST-2 dataset for **binary sentiment analysis**.\n",
    "\n",
    "- **Task:** `text-classification`\n",
    "- **Classes:** `POSITIVE` or `NEGATIVE`\n",
    "- **Input:** A string of text (e.g., a product review or tweet)\n",
    "- **Output:** A label with a confidence score\n",
    "\n",
    "We'll deploy this model using the **Hugging Face prebuilt container** for SageMaker, which supports `transformers`-based models with no need to build custom Docker images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb8055",
   "metadata": {},
   "source": [
    "\n",
    "## About the Model Page on Hugging Face Hub\n",
    "\n",
    "You can view this model live on the Hugging Face website:\n",
    "\n",
    "**[distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english)**\n",
    "\n",
    "This page gives you everything you need to use the model:\n",
    "\n",
    "---\n",
    "\n",
    "### What You‚Äôll Find on the Page\n",
    "\n",
    "- **Model card**:  \n",
    "  Includes an overview of what the model does, the dataset it was trained on (SST-2), and the intended use.\n",
    "\n",
    "- **Widget**:  \n",
    "  You can test the model directly in your browser by entering text and getting a sentiment result (`POSITIVE` or `NEGATIVE`).\n",
    "\n",
    "- **Usage code**:  \n",
    "  Shows how to load and run the model using `transformers` in Python.\n",
    "\n",
    "  Example:\n",
    "  ```python\n",
    "  from transformers import pipeline\n",
    "  classifier = pipeline(\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "  classifier(\"This is awesome!\")\n",
    "  ```\n",
    "\n",
    "- **Tags**:  \n",
    "  Tells you this model supports tasks like `text-classification`, and that it runs on `pytorch`.\n",
    "\n",
    "- **Model files**:  \n",
    "  Shows the underlying files (config, tokenizer, weights) that SageMaker will automatically pull when deploying.\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Page Matters\n",
    "\n",
    "- Hugging Face model pages are the **source of truth** for model metadata.\n",
    "- You‚Äôll reference these pages often when building GenAI pipelines in production.\n",
    "- SageMaker‚Äôs Hugging Face container can **deploy any model from the hub using just this model ID**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef748e4",
   "metadata": {},
   "source": [
    "## Deploy the Hugging Face Model to SageMaker\n",
    "\n",
    "Now that we‚Äôve chosen our model (`distilbert-base-uncased-finetuned-sst-2-english`), we‚Äôll deploy it using the **SageMaker HuggingFaceModel class**, which makes it easy to host models from the ü§ó Hub without custom Docker images.\n",
    "\n",
    "We specify:\n",
    "- The model ID from Hugging Face\n",
    "- The task (e.g., `text-classification`)\n",
    "- The runtime environment (Transformers, PyTorch, Python)\n",
    "- The IAM role to grant SageMaker permissions\n",
    "\n",
    "Then we deploy the model to a managed SageMaker endpoint.\n",
    "\n",
    "---\n",
    "\n",
    "### What Happens Behind the Scenes?\n",
    "\n",
    "- SageMaker pulls a prebuilt container image for Hugging Face Transformers.\n",
    "- It downloads the model weights and tokenizer from the ü§ó Hub.\n",
    "- It spins up a new endpoint (`ml.t2.medium`) for real-time inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Notes\n",
    "- This model performs binary sentiment analysis (`POSITIVE` or `NEGATIVE`).\n",
    "- The endpoint will incur cost while it is running ‚Äî delete it when you're done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28deceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Ziggy\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n",
      "----------!"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "roleArn = {o[\"OutputKey\"]: o[\"OutputValue\"] for o in outputs if o[\"OutputKey\"] == \"RoleArn\"}[\"RoleArn\"]\n",
    "\n",
    "# Define model configuration\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    'HF_TASK':'text-classification'\n",
    "}\n",
    "\n",
    "# Create HuggingFaceModel object\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.26',\n",
    "    pytorch_version='1.13',\n",
    "    py_version='py39',\n",
    "    env=hub,\n",
    "    role=roleArn\n",
    ")\n",
    "\n",
    "# Deploy the model to an endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    endpoint_name='huggingface-text-endpoint'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e1325",
   "metadata": {},
   "source": [
    "## Trigger the Deployed Endpoint via Boto3\n",
    "\n",
    "Once your model is deployed to a SageMaker real-time endpoint, you can send it input using the `boto3` SageMaker Runtime client.\n",
    "\n",
    "---\n",
    "\n",
    "### Function Overview\n",
    "\n",
    "We define a reusable function: `trigger_huggingface_text_endpoint()`\n",
    "\n",
    "- It takes in a **text prompt**\n",
    "- Sends the prompt to the deployed endpoint using `invoke_endpoint()`\n",
    "- Parses and returns the JSON prediction result\n",
    "\n",
    "---\n",
    "\n",
    "### Input Format\n",
    "\n",
    "The input must be structured as:\n",
    "```json\n",
    "{ \"inputs\": \"Your text here\" }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66773a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "endpoint_name = \"huggingface-text-endpoint\"\n",
    "\n",
    "# Create the runtime client\n",
    "runtime = boto3.client(\"sagemaker-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "def trigger_huggingface_text_endpoint(runtime, prompt):\n",
    "\n",
    "    # Prepare the input\n",
    "    payload = {\n",
    "        \"inputs\": prompt\n",
    "    }\n",
    "\n",
    "\n",
    "    # Call the endpoint\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "\n",
    "    # Parse and print the result\n",
    "    result = json.loads(response['Body'].read())\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64962c4e",
   "metadata": {},
   "source": [
    "## Inference Examples: Sentiment in Action\n",
    "\n",
    "After deploying the Hugging Face model and invoking it using our function, we tested several real-world phrases to observe how the model classifies sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 1: Positive Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3392efc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998821020126343}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = trigger_huggingface_text_endpoint(runtime, \"This class is amazing!\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff4279",
   "metadata": {},
   "source": [
    "‚úÖ The model confidently detects strong positive emotion.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2: Negative Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "343a0980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997609257698059}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = trigger_huggingface_text_endpoint(runtime, \"This class is horrible!\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c557586",
   "metadata": {},
   "source": [
    "‚úÖ Correctly detects the strong negative tone with high confidence.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 3: Informal/Slang Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dc4b71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9928885102272034}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = trigger_huggingface_text_endpoint(runtime, \"This class is the bomb!\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cdb74c",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Why this is interesting**:  \n",
    "Although the phrase is slang for something very good, the model misclassifies it due to lack of context or training on informal expressions.\n",
    "\n",
    "---\n",
    "\n",
    "### Teaching Point\n",
    "\n",
    "Pretrained models work best with **standard, literal phrasing**. If your audience uses informal or domain-specific language, you may need to:\n",
    "- Fine-tune the model\n",
    "- Add post-processing logic\n",
    "- Use few-shot prompting (if using a foundation model instead)\n",
    "\n",
    "This is a great opportunity to talk about **limitations of out-of-the-box models** and the value of customization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbdd98",
   "metadata": {},
   "source": [
    "## Where is it running?\n",
    "\n",
    "- Model image is pulled from ECR\n",
    "- Weights are downloaded from Hugging Face Hub\n",
    "- Inference runs inside a SageMaker-managed container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075028f",
   "metadata": {},
   "source": [
    "## Cleanup: Delete the Endpoint and Configuration\n",
    "\n",
    "After testing your SageMaker-hosted model, it‚Äôs important to delete the endpoint and its configuration to avoid ongoing charges.\n",
    "\n",
    "SageMaker real-time endpoints are **always-on** and billed by the hour, even if you‚Äôre not sending traffic.\n",
    "\n",
    "---\n",
    "\n",
    "### What to Delete\n",
    "\n",
    "1. **The Endpoint** ‚Äì This is the live model you invoked.\n",
    "2. **The Endpoint Configuration** ‚Äì This defines the settings (instance type, model, etc.).\n",
    "\n",
    "Both usually share the same name unless explicitly renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "565314b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted endpoint: huggingface-text-endpoint\n",
      "Deleted endpoint config: huggingface-text-endpoint\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client(\"sagemaker\", region_name=\"us-east-1\")\n",
    "endpoint_name = \"huggingface-text-endpoint\"\n",
    "\n",
    "# Delete the endpoint\n",
    "try:\n",
    "    client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"Deleted endpoint: {endpoint_name}\")\n",
    "except client.exceptions.ClientError as e:\n",
    "    print(f\"Could not delete endpoint: {e}\")\n",
    "\n",
    "# Delete the endpoint config (same name as endpoint if not renamed)\n",
    "try:\n",
    "    client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "    print(f\"Deleted endpoint config: {endpoint_name}\")\n",
    "except client.exceptions.ClientError as e:\n",
    "    print(f\"Could not delete endpoint config: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b5a79",
   "metadata": {},
   "source": [
    "## Cleanup: Delete the CloudFormation Stack\n",
    "\n",
    "If you created infrastructure (like IAM roles or S3 buckets) using a CloudFormation template, you should also clean it up once you're done.\n",
    "\n",
    "The script below deletes your entire CloudFormation stack and waits for the deletion to complete.\n",
    "\n",
    "---\n",
    "\n",
    "### What This Deletes\n",
    "\n",
    "- IAM roles\n",
    "- S3 buckets\n",
    "- SageMaker-related permissions\n",
    "- Any other resources created in the stack\n",
    "\n",
    "Be careful ‚Äî this will **permanently delete** all resources provisioned by the stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea238cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß® Deleting stack: caip04-cloud-ml-stack\n",
      "‚è≥ Waiting for stack to be fully deleted...\n",
      "‚úÖ Stack 'caip04-cloud-ml-stack' successfully deleted.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "cf = boto3.client(\"cloudformation\")\n",
    "stack_name = \"caip04-cloud-ml-stack\"\n",
    "\n",
    "def delete_stack_and_wait(stack_name):\n",
    "    print(f\"üß® Deleting stack: {stack_name}\")\n",
    "    \n",
    "    # Initiate deletion\n",
    "    cf.delete_stack(StackName=stack_name)\n",
    "\n",
    "    # Wait until stack deletion is complete\n",
    "    waiter = cf.get_waiter(\"stack_delete_complete\")\n",
    "    print(\"‚è≥ Waiting for stack to be fully deleted...\")\n",
    "    waiter.wait(StackName=stack_name)\n",
    "\n",
    "    print(f\"‚úÖ Stack '{stack_name}' successfully deleted.\")\n",
    "\n",
    "# Call it\n",
    "delete_stack_and_wait(stack_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b829aa-5d6a-41d3-a730-3c02e1afb1ea",
   "metadata": {},
   "source": [
    "## Wrap-Up & Takeaways "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda14a9a-0efd-4b36-bcd3-6275f515ada5",
   "metadata": {},
   "source": [
    "## In This Notebook\n",
    "\n",
    "In this notebook, you deployed an open-source Hugging Face model to Amazon SageMaker for real-time inference. You:\n",
    "\n",
    "- Explored the Hugging Face model hub and selected a text classification model\n",
    "- Used SageMaker‚Äôs HuggingFaceModel class to deploy the model without Docker\n",
    "- Sent test inputs using `boto3` directly\n",
    "- Reviewed inference results and observed model behavior on varied input\n",
    "- Cleaned up your endpoint and CloudFormation stack to avoid charges\n",
    "\n",
    "---\n",
    "\n",
    "## This Workflow Reflects What Real ML Teams Do in Production\n",
    "\n",
    "- Deploy pre-trained models to scalable inference endpoints\n",
    "- Send payloads via APIs and automate feedback pipelines\n",
    "- Monitor results and validate predictions against real user input\n",
    "- Perform cost-aware infrastructure teardown when workloads are complete\n",
    "- Use model hubs like Hugging Face to accelerate experimentation and delivery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3667709e",
   "metadata": {},
   "source": [
    "## What This Looks Like in Industry: MLOps + DevOps\n",
    "\n",
    "While this notebook walks through deploying a single Hugging Face model manually, production-grade systems automate and extend this process through **MLOps** and **DevOps** practices.\n",
    "\n",
    "---\n",
    "\n",
    "### MLOps Practices\n",
    "\n",
    "- **Model Versioning**:  \n",
    "  Every model (and dataset) is versioned and stored ‚Äî often using S3, Git, or tools like MLflow or SageMaker Model Registry.\n",
    "\n",
    "- **Continuous Training Pipelines**:  \n",
    "  Training is automated via pipelines (e.g., SageMaker Pipelines, Airflow, or Kubeflow) triggered by data drift, retraining schedules, or performance degradation.\n",
    "\n",
    "- **Endpoint Monitoring**:  \n",
    "  Production endpoints are monitored using tools like CloudWatch, Datadog, or Prometheus to detect latency spikes, input anomalies, or accuracy drops.\n",
    "\n",
    "- **Shadow Testing + A/B Testing**:  \n",
    "  New models are deployed alongside existing ones to evaluate performance in live environments before being fully promoted.\n",
    "\n",
    "---\n",
    "\n",
    "### DevOps Practices\n",
    "\n",
    "- **Infrastructure as Code (IaC)**:  \n",
    "  Everything ‚Äî IAM roles, S3 buckets, models, endpoints ‚Äî is created via templates (e.g., CloudFormation, Terraform, or CDK).\n",
    "\n",
    "- **CI/CD for Models**:  \n",
    "  GitHub Actions, CodePipeline, or Jenkins are used to deploy models automatically when code or config changes are pushed.\n",
    "\n",
    "- **Environment Management**:  \n",
    "  Teams manage dev, staging, and prod environments separately using tags, branch-based workflows, or isolated accounts.\n",
    "\n",
    "- **Cost and Resource Controls**:  \n",
    "  Autoscaling, scheduled shutdowns, and tagging help reduce cost and improve visibility across environments.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Thought\n",
    "\n",
    "In this notebook, you walked through the **manual, educational path** of deploying and testing a Hugging Face model. In industry, these same steps are wrapped in automation, monitoring, and governance ‚Äî enabling teams to scale GenAI reliably and securely.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
