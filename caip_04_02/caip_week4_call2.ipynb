{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763c0e9a-2387-493f-94bc-96ffecd4a9c8",
   "metadata": {},
   "source": [
    "## Certified AI Practitioner Week 04 Call 02 - From ðŸ¤— to Chatbot: Open-Source AI in Production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84b897-7cba-4d9e-920f-ca6193b7e4ad",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880bca35-6340-4db1-9b2b-760da3539d1f",
   "metadata": {},
   "source": [
    "- Understand what conversational AI is and how `DialoGPT` enables it  \n",
    "- Load and use a Hugging Face chatbot model locally with the `transformers` library  \n",
    "- Maintain conversation history to support multi-turn dialogue  \n",
    "- Identify trade-offs between different deployment architectures (e.g., Lex vs SageMaker)  \n",
    "- Discuss real-world considerations for chatbot safety, scalability, and cost  \n",
    "- Connect local development with what production-ready chatbot systems look like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ef3ff",
   "metadata": {},
   "source": [
    "## Example Model: `microsoft/DialoGPT-medium`\n",
    "### Explore the Model on Hugging Face  \n",
    "\n",
    "`DialoGPT` is a conversational model fine-tuned from GPT-2 by Microsoft for multi-turn dialogue. Itâ€™s optimized to generate human-like responses in chat-style applications.\n",
    "\n",
    "**Model Card:**  \n",
    "â†’ [View on Hugging Face](https://huggingface.co/microsoft/DialoGPT-medium)\n",
    "\n",
    "\n",
    "### Conversation Behavior\n",
    "\n",
    "This model expects:\n",
    "- **Sequential prompts** where each response depends on prior inputs  \n",
    "- **Manual context handling** (i.e., feed the entire conversation history for coherent replies)\n",
    "\n",
    "### Try It in the Browser + Explore Spaces\n",
    "\n",
    "On the model page, scroll down to the **\"Hosted inference API\"** to test the model live (if available).  \n",
    "Youâ€™ll also find a list of community-built **Spaces** that use this model â€” from basic chatbot UIs to creative applications.\n",
    "\n",
    "> Some Spaces may be experimental or contain NSFW content â€” see the note below on community moderation.\n",
    "\n",
    "---\n",
    "\n",
    "> **âš ï¸ Note on Hugging Face Community Content**  \n",
    "> Hugging Face hosts open-source models and apps (\"Spaces\") contributed by the global ML community. While this enables rapid innovation and access to cutting-edge tools, it also means some content may be unmoderated or NSFW.  \n",
    ">  \n",
    "> In production environments, it's critical to:\n",
    "> - Review model cards and licenses  \n",
    "> - Validate model behavior before deploying  \n",
    "> - Apply content moderation and filtering when necessary  \n",
    "> - Prefer verified models for enterprise use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e8510",
   "metadata": {},
   "source": [
    "## Set Up Your Environment\n",
    "\n",
    "Before using the DialoGPT model, you'll need to install the required libraries and import necessary modules.\n",
    "\n",
    "```python\n",
    "# Install Hugging Face transformers if not already installed\n",
    "!pip install -q transformers\n",
    "\n",
    "# Import core libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "```\n",
    "\n",
    "> **Note:** DialoGPT is a PyTorch-based model, so weâ€™ll be using the PyTorch version of Hugging Faceâ€™s `transformers` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147df883",
   "metadata": {},
   "source": [
    "## Load the Chatbot Model\n",
    "\n",
    "We'll load the `microsoft/DialoGPT-medium` model along with its tokenizer. This model is optimized for conversational, multi-turn dialogue.\n",
    "\n",
    "```python\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "```\n",
    "\n",
    "> **Note:** This loads both the tokenizer (which converts text to tokens) and the model (which generates responses) from Hugging Faceâ€™s model hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f336064e",
   "metadata": {},
   "source": [
    "## Build a Local Conversational Loop\n",
    "\n",
    "Now let's build a simple interactive chatbot loop using `DialoGPT`.  \n",
    "This script lets you chat with the model in real time, maintaining conversation history.\n",
    "\n",
    "```python\n",
    "# Initialize conversation history\n",
    "chat_history_ids = None\n",
    "attention_mask = None\n",
    "\n",
    "print(\"Chatbot is ready! Type 'quit' to exit or 'reset' to clear conversation history.\")\n",
    "for step in range(5):  # Adjust as needed\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    if user_input.lower() == \"reset\":\n",
    "        chat_history_ids = None\n",
    "        attention_mask = None\n",
    "        print(\"Chat history cleared.\")\n",
    "        continue\n",
    "\n",
    "    # Encode input\n",
    "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    new_attention_mask = torch.ones_like(new_input_ids)\n",
    "\n",
    "    # Combine with chat history\n",
    "    if chat_history_ids is not None:\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, new_attention_mask], dim=-1)\n",
    "    else:\n",
    "        bot_input_ids = new_input_ids\n",
    "        attention_mask = new_attention_mask\n",
    "\n",
    "    # Generate response with attention mask\n",
    "    output_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=1000,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "    # Update history\n",
    "    chat_history_ids = output_ids\n",
    "    attention_mask = torch.ones_like(chat_history_ids)\n",
    "\n",
    "    # Decode and print reply\n",
    "    response = tokenizer.decode(output_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    print(f\"Bot: {response}\")\n",
    "```\n",
    "\n",
    "> This loop maintains conversational context by appending each input to a running history. The model sees the entire dialogue, making its replies more coherent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c926a1a",
   "metadata": {},
   "source": [
    "## Clean Up the Conversation History (Optional)\n",
    "\n",
    "In real-world chatbot applications, you may want to reset or manage the conversation history based on session limits, user actions, or context drift.\n",
    "\n",
    "You can clear the context at any time by resetting `chat_history_ids`:\n",
    "\n",
    "```python\n",
    "# Reset the chat history (e.g., after N turns or when the user types 'reset')\n",
    "chat_history_ids = None\n",
    "print(\"Chat history cleared.\")\n",
    "```\n",
    "> Tip: In production, you might also limit the total token count or summarize long histories to reduce input size and cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b654a9",
   "metadata": {},
   "source": [
    "## Bonus: Where This Fits in Production\n",
    "\n",
    "Building a chatbot with `DialoGPT` locally is a great proof of concept â€” but how do real teams scale this?\n",
    "\n",
    "In production, this model could be:\n",
    "\n",
    "- **Packaged in a Docker container**\n",
    "- **Hosted on SageMaker, ECS, or Lambda with EFS**\n",
    "- **Accessed via API Gateway or a chatbot UI**\n",
    "- **Integrated with a frontend (like a website or support chat)**\n",
    "\n",
    "Youâ€™d also want to:\n",
    "\n",
    "- Add authentication and throttling  \n",
    "- Log requests/responses for review  \n",
    "- Add safety filters and tone moderation  \n",
    "- Control cost by scaling down idle services\n",
    "\n",
    "> Remember: A working chatbot â‰  a production-ready product. Think about user safety, monitoring, and cost control from day one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a303e",
   "metadata": {},
   "source": [
    "## Cost-Aware Architecture Decisions\n",
    "\n",
    "When deploying chatbots, itâ€™s easy to overbuild for scale that may never come. In most real-world use cases, **simplicity saves money** â€” especially during early development or internal pilots.\n",
    "\n",
    "### Option A: Hugging Face + SageMaker\n",
    "- Pay for **instance time** (e.g., `ml.t2.medium`, `ml.g5.xlarge`)\n",
    "- Good for custom models or full control over inference\n",
    "- Requires MLOps setup: IAM, monitoring, scaling, shutdowns\n",
    "\n",
    "### Option B: Amazon Lex (NLU-based Chatbot)\n",
    "- Pay **only for usage** (per message)\n",
    "- Easy UI builder, built-in slot filling + Lambda hooks\n",
    "- Best for forms, menus, support bots with clear flows\n",
    "\n",
    "---\n",
    "\n",
    "### Comparing Costs\n",
    "\n",
    "| Architecture      | Best For                    | Cost Model          | Example |\n",
    "|-------------------|-----------------------------|----------------------|---------|\n",
    "| SageMaker + ðŸ¤—     | Custom LLMs, GenAI chat     | Per hour (instance) | ~$0.05â€“$1/hr |\n",
    "| Amazon Lex        | Menus, forms, call centers  | Per request (usage) | ~$4/month for low usage |\n",
    "\n",
    "> Most startups and internal teams donâ€™t get millions of chats per day.  \n",
    "> Start small, monitor usage, and only scale when needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b829aa-5d6a-41d3-a730-3c02e1afb1ea",
   "metadata": {},
   "source": [
    "## Wrap-Up & Takeaways "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda14a9a-0efd-4b36-bcd3-6275f515ada5",
   "metadata": {},
   "source": [
    "## In This Notebook\n",
    "\n",
    "In this notebook, you explored how to build and test a conversational AI model locally using Hugging Faceâ€™s `DialoGPT`. You:\n",
    "\n",
    "- Explored the `DialoGPT-medium` model on Hugging Face and reviewed key metadata  \n",
    "- Loaded the model and tokenizer locally with the `transformers` library  \n",
    "- Built a conversational loop that maintains chat history  \n",
    "- Learned how to reset context and handle multi-turn dialogue  \n",
    "- Discussed production architecture options and cost-aware deployment strategies  \n",
    "\n",
    "---\n",
    "\n",
    "## This Workflow Reflects What Real ML Teams Do in Production\n",
    "\n",
    "- Evaluate open-source models before committing to deployment  \n",
    "- Test model behavior locally using realistic user inputs  \n",
    "- Consider security, moderation, and cost early in the design process  \n",
    "- Build APIs and chat interfaces around pretrained models  \n",
    "- Choose between managed services (Lex) and custom hosting (SageMaker) based on scale and use case\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
